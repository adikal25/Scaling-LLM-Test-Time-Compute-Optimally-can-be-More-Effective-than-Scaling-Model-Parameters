# Scaling LLM Test-Time Compute Optimally

[![arXiv](https://img.shields.io/badge/arXiv-2408.03314-b31b1b.svg)](https://arxiv.org/abs/2408.03314)

> **Authors:** Charlie Snell¬π, Jaehoon Lee¬≤, Kelvin Xu¬≤, Aviral Kumar¬≤  
> **Affiliations:** ¬πUC Berkeley, ¬≤Google DeepMind  
> **Date:** August 7, 2024

---

## üìã Table of Contents
- [Overview](#overview)
- [Key Findings](#key-findings)
- [Core Concepts](#core-concepts)
- [Methodology](#methodology)
- [Results Summary](#results-summary)
- [Architecture Components](#architecture-components)
- [Experimental Setup](#experimental-setup)
- [Implementation Details](#implementation-details)
- [Citation](#citation)

---

## üéØ Overview

This paper investigates a fundamental question in LLM deployment: **Can we make smaller models perform as well as larger ones by using more computation at inference time?**

### The Core Insight
Just like humans spend more time thinking through difficult problems versus easy ones, LLMs should adaptively allocate compute based on problem difficulty. The study demonstrates that strategically allocating test-time compute can be **4√ó more efficient** than naive approaches and, in some cases, a smaller model with additional test-time compute can **outperform a 14√ó larger model**.

### Central Question
> *If an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt?*

### Why This Matters
- **Deployment Efficiency**: Use smaller on-device models instead of datacenter-scale LLMs
- **Self-Improvement**: Path toward general self-improvement algorithms with reduced human supervision
- **Cost Optimization**: Strategic tradeoff between pretraining and inference costs
- **Practical Impact**: Shift from "bigger models" to "smarter inference" paradigm

---

## üîë Key Findings

### 1. Compute-Optimal Scaling Strategy

**The Human Analogy**: When humans take exams, they handle problems differently:
- **Easy problems**: Quick check and minor corrections (like revisions)
- **Medium problems**: Try a few different approaches, refine the best one
- **Hard problems**: Must explore many fundamentally different strategies
- **Beyond capability**: Need to learn more first (more pretraining)

**LLM Strategy Mapping:**

| Difficulty | Pass@1 | Optimal Strategy | Ratio (Seq:Par) | Why It Works |
|------------|--------|------------------|-----------------|--------------|
| **EASY** | >60% | Pure sequential revisions | 128:1 | Model is "close" - just needs refinement. Already has right approach, minor errors to fix. |
| **MEDIUM** | 30-60% | Balanced mix | 16:8 to 32:4 | Sometimes right track (revise), sometimes wrong approach (restart). |
| **HARD** | 10-30% | Mostly parallel search | 4:32 to 8:16 | Need to explore fundamentally different solution strategies. Verifier-guided search helps. |
| **VERY HARD** | <10% | Pretraining needed | N/A | Beyond model's capability - test-time compute can't bridge knowledge gap. |

**Result:** Compute-optimal allocation achieves **4√ó efficiency gains** over best-of-N baseline by matching strategy to difficulty

### 2. Test-Time vs. Pretraining Compute Tradeoff

**The Fundamental Question**: Given extra compute budget, should you:
- (A) Train a 14√ó larger model, OR
- (B) Use the same FLOPs for test-time compute with a smaller model?

**The Answer Depends on Three Factors:**

#### Factor 1: Inference Load Ratio (R)
```
R = D_inference / D_pretrain
  = (inference tokens) / (pretraining tokens)

R << 1: Self-improvement pipelines (generate training data)
R ‚âà 1:  Balanced usage
R >> 1: Production deployments (millions of users)
```

#### Factor 2: Question Difficulty Distribution

#### Factor 3: Base Model Capability

**Decision Matrix:**

| Scenario | R Value | Difficulty | Winner | Why |
|----------|---------|------------|--------|-----|
| Self-improvement | R << 1 | Easy/Medium | üü¢ Test-time compute | Inference cost is small fraction; can afford exploration |
| Self-improvement | R << 1 | Hard | üî¥ Pretraining | Still need better base model |
| Production | R >> 1 | Easy | üü° Test-time (marginal) | Inference cost dominates, but simple problems benefit |
| Production | R >> 1 | Medium/Hard | üî¥ Pretraining | Inference cost too high per query |
| Balanced | R ‚âà 1 | Easy | üü¢ Test-time compute | Good efficiency |
| Balanced | R ‚âà 1 | Medium | üü° Mixed | Depends on specifics |
| Balanced | R ‚âà 1 | Hard | üî¥ Pretraining | Need capability boost |

**FLOPS Calculation:**
```
Larger Model (14√ó params):
  Pretrain: 6 √ó (14N) √ó D_pretrain = 84N √ó D_pretrain
  Inference: 2 √ó (14N) √ó D_inference = 28N √ó D_inference
  TOTAL: 14 √ó (6N√óD_pretrain + 2N√óD_inference)

Smaller Model + Test-Time:
  Pretrain: 6N √ó D_pretrain  (fixed)
  Inference: 2N √ó D_inference √ó (samples)  (variable)
  
To match FLOPs:
  samples = 14 + 42R(14-1) = 14 + 546R
  
Example:
  R = 0.1  ‚Üí 69 samples
  R = 1.0  ‚Üí 560 samples
  R = 10   ‚Üí 5,474 samples
```

### 3. Quantitative Performance Gains

**Revisions (Sequential Refinement):**
- Easy questions: +27.8% relative improvement
- Medium questions: +11.8% to +16.7%
- Hard questions: -24.3% to -37.2% (pretraining better)

**PRM Search (Parallel Exploration):**
- Easy questions: +2.0% to +2.2%
- Medium questions: +19.1%
- Hard questions: -30.6% to -52.9% (pretraining better)

---

## üß† Core Concepts

### Unified Framework: Proposer + Verifier

All test-time compute methods can be understood through two components:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Test-Time Compute = Proposer + Verifier        ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  Proposer: How to generate candidate solutions  ‚îÇ
‚îÇ  Verifier: How to evaluate and select best      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Two Primary Mechanisms

#### 1Ô∏è‚É£ Refining the Proposal Distribution

**Approach:** Modify how the model generates responses

**Algorithm 1: Sequential Revision Generation**
```
Input: prompt q, revision model M_rev, number of revisions n, context size k
Output: Best answer from revision chain

1: answers ‚Üê []
2: context ‚Üê q
3: for i = 1 to n do
4:     answer_i ‚Üê M_rev.Generate(context)
5:     answers.Append(answer_i)
6:     context ‚Üê BuildContext(q, answers[max(0, i-k):i])  // Keep last k answers
7: end for
8: return SelectBest(answers, verifier)
```

**Algorithm 2: Training Data Generation for Revision Models**
```
Input: Training questions Q, base model M, samples per question S
Output: Revision trajectories T

1: T ‚Üê []
2: for each q ‚àà Q do
3:     samples ‚Üê M.Generate(q, n=S, temp=0.8)
4:     correct ‚Üê {s ‚àà samples | IsCorrect(s, q)}
5:     incorrect ‚Üê {s ‚àà samples | ¬¨IsCorrect(s, q)}
6:     for each c ‚àà correct do
7:         k ‚Üê Uniform({0, 1, 2, 3, 4})  // Trajectory length
8:         if k > 0 then
9:             distances ‚Üê [EditDistance(inc, c) for inc ‚àà incorrect]
10:            last_inc ‚Üê incorrect[ArgMin(distances)]  // Most similar error
11:            others ‚Üê RandomSample(incorrect \ {last_inc}, k-1)
12:            trajectory ‚Üê [others, last_inc, c]
13:        else
14:            trajectory ‚Üê [c]
15:        end if
16:        T.Append((q, trajectory))
17:    end for
18: end for
19: return T
```

**Key Insight:** Sequential revisions ‚âà local refinement. Works when model is "close" to correct answer.

#### 2Ô∏è‚É£ Optimizing Against Verifiers

**Approach:** Search over candidate space using reward model

**Process Reward Model (PRM):**
- Predicts correctness at each solution step
- Trained on Monte Carlo rollouts (no human labels needed)
- Enables step-wise guidance during search

**Algorithm 3: Best-of-N Weighted Selection**
```
Input: Samples S, verifier V
Output: Best final answer

1: answer_scores ‚Üê {}  // Map from final answer to total score
2: for each s ‚àà S do
3:     final_ans ‚Üê ExtractFinalAnswer(s)
4:     score ‚Üê V.Score(s)
5:     answer_scores[final_ans] ‚Üê answer_scores[final_ans] + score
6: end for
7: return argmax_{ans} answer_scores[ans]
```

**Algorithm 4: Beam Search with PRM**
```
Input: Question q, base model M, PRM verifier V, budget N, beam width M_width
Output: Best solution

1: beams ‚Üê M.SampleStep(q, n=N)  // Initial step samples
2: for step = 1 to MAX_STEPS do
3:     if AllComplete(beams) then break
4:     // Score all current beams
5:     scores ‚Üê [V.ScoreStep(b) for b ‚àà beams]
6:     // Keep top N/M_width beams
7:     top_beams ‚Üê SelectTop(beams, scores, k=N/M_width)
8:     // Expand each by M_width branches
9:     new_beams ‚Üê []
10:    for each b ‚àà top_beams do
11:        branches ‚Üê M.ContinueFrom(b, n=M_width)
12:        new_beams.Extend(branches)
13:    end for
14:    beams ‚Üê new_beams
15: end for
16: return BestOfNWeighted(beams, V)
```

**Algorithm 5: Lookahead Search**
```
Input: Question q, model M, PRM V, budget N, beam width M, lookahead k
Output: Best solution

1: beams ‚Üê M.SampleStep(q, n=N)
2: for step = 1 to MAX_STEPS do
3:     if AllComplete(beams) then break
4:     // For each beam, simulate k steps ahead
5:     scores ‚Üê []
6:     for each b ‚àà beams do
7:         rollout ‚Üê M.Rollout(b, k_steps=k, temp=0)  // Deterministic
8:         score ‚Üê V.ScoreStep(rollout[-1])  // Score end of rollout
9:         scores.Append(score)
10:    end for
11:    // Select and expand as in beam search
12:    top_beams ‚Üê SelectTop(beams, scores, k=N/M)
13:    beams ‚Üê [b for beam ‚àà top_beams for b ‚àà M.ContinueFrom(beam, n=M)]
14: end for
15: return BestOfNWeighted(beams, V)

Note: Cost = N √ó (k + 1) generations
```

**Key Insight:** Parallel search ‚âà global exploration. Works when need to try fundamentally different approaches.

### The Compute-Optimal Strategy Formula

**Objective:** Maximize accuracy given compute budget N

```
Œ∏*_{q,y*(q)}(N) = argmax E[1_{y=y*(q)}]
                    Œ∏    y~Target(Œ∏,N,q)

where:
  Œ∏ = hyperparameters (sequential/parallel ratio, search algorithm)
  N = compute budget (number of generations)
  q = the question/prompt
  y*(q) = ground truth answer
  Target(Œ∏,N,q) = distribution over outputs given strategy Œ∏
```

**Algorithm 6: Difficulty Estimation (Oracle)**
```
Input: Model M, questions Q, num_samples n
Output: Difficulty bins for each question

1: difficulties ‚Üê {}
2: for each q ‚àà Q do
3:     samples ‚Üê M.Generate(q, n=n)
4:     pass_rate ‚Üê Œ£ IsCorrect(s, q) / n for s ‚àà samples
5:     difficulties[q] ‚Üê pass_rate
6: end for
7: bins ‚Üê AssignQuantiles(difficulties, n_bins=5)
8: return bins
```

**Algorithm 7: Difficulty Estimation (Model-Predicted)**
```
Input: Model M, verifier V, questions Q, num_samples n
Output: Difficulty bins (no ground truth needed)

1: difficulties ‚Üê {}
2: for each q ‚àà Q do
3:     samples ‚Üê M.Generate(q, n=n)
4:     avg_score ‚Üê Mean([V.Score(s) for s ‚àà samples])
5:     difficulties[q] ‚Üê avg_score
6: end for
7: bins ‚Üê AssignQuantiles(difficulties, n_bins=5)
8: return bins
```

**Algorithm 8: Compute-Optimal Strategy Selection**
```
Input: Difficulty d, budget N, method ‚àà {revisions, prm_search}
Output: Strategy parameters Œ∏

1: if method = "revisions" then
2:     if d = "EASY" then
3:         return {seq: N, par: 1}  // Pure sequential
4:     else if d = "MEDIUM" then
5:         return {seq: N/4, par: 4}  // 4:1 ratio
6:     else if d = "HARD" then
7:         return {seq: N/16, par: 16}  // 1:4 ratio
8:     else  // VERY_HARD
9:         return {seq: 1, par: N}  // Pure parallel
10:    end if
11: else if method = "prm_search" then
12:    if N < 32 then  // Low budget
13:        if d ‚àà {"MEDIUM", "HARD"} then
14:            return "beam_search"
15:        else
16:            return "best_of_n"
17:        end if
18:    else  // High budget
19:        if d = "EASY" then
20:            return "best_of_n"  // Avoid over-optimization
21:        else
22:            return "beam_search"
23:        end if
24:    end if
25: end if
```

---

## üî¨ Methodology

### Question Difficulty Estimation

Two approaches for binning questions into 5 difficulty levels:

#### Oracle Difficulty (for analysis)
**Algorithm: Oracle Difficulty Estimation**
```
Input: Model M, questions Q, n_samples = 2048
Output: Difficulty bins D

1: for each q ‚àà Q do
2:     samples ‚Üê M.Generate(q, n=n_samples)
3:     pass_rate[q] ‚Üê |{s ‚àà samples : IsCorrect(s, q)}| / n_samples
4: end for
5: D ‚Üê PartitionIntoQuantiles(pass_rate, n_bins=5)
6: return D
```

#### Predicted Difficulty (for deployment)
**Algorithm: Model-Predicted Difficulty Estimation**
```
Input: Model M, verifier V, questions Q, n_samples = 2048
Output: Difficulty bins D (no ground truth needed)

1: for each q ‚àà Q do
2:     samples ‚Üê M.Generate(q, n=n_samples)
3:     scores ‚Üê [V.Score(s) for s ‚àà samples]
4:     avg_score[q] ‚Üê Mean(scores)
5: end for
6: D ‚Üê PartitionIntoQuantiles(avg_score, n_bins=5)
7: return D
```

**Note:** Difficulty estimation has overhead; future work should explore cheaper methods (e.g., finetuned difficulty predictors).

### Search Algorithms Compared

**The Core Trade-off**: Exploration vs. Exploitation of Verifier Signal

| Algorithm | Strategy Type | Compute Cost | Strengths | Weaknesses | Best For |
|-----------|---------------|--------------|-----------|------------|----------|
| **Best-of-N** | Parallel exploration | N gens | Robust, doesn't over-optimize | No step-wise guidance | Easy problems, high budgets |
| **Beam Search** | Greedy with breadth | N gens | Efficient search, step-wise guidance | Can over-optimize PRM | Medium problems, low budgets |
| **Lookahead** | Forward simulation | N√ó(k+1) gens | Better value estimates | Very expensive | Theoretically best (practically: too costly) |

**Key Insight from Results**: Lookahead search (theoretically strongest) actually underperforms due to compute cost. The rollout overhead makes it less efficient than beam search at the same budget.

#### Critical Finding: Over-Optimization Problem

**What Happens**: Stronger optimizers (beam search, lookahead) can exploit spurious patterns in the verifier.

**Evidence by Difficulty**:

```
Generation Budget: 4 ‚Üí 256

EASY Problems (Levels 1-2):
  ‚îú‚îÄ Low Budget (4-16):   Beam Search >> Best-of-N  ‚úì
  ‚îî‚îÄ High Budget (64-256): Beam Search < Best-of-N   ‚úó (over-optimization)

MEDIUM Problems (Levels 3-4):
  ‚îî‚îÄ All Budgets:          Beam Search ‚â• Best-of-N  ‚úì

HARD Problems (Level 5):
  ‚îî‚îÄ All Budgets:          Both struggle equally     ~
```

**Why This Happens**:

1. **Easy problems**: Verifier is mostly correct ‚Üí beam search finds adversarial edge cases that fool verifier
2. **Hard problems**: Base model rarely generates good candidates ‚Üí search helps find them, verifier errors less critical
3. **Failure modes observed**:
   - Repetitive low-information steps (e.g., "We continue by applying the formula...")
   - Overly short solutions (1-2 steps claiming completion)
   - Exploitation of verifier training distribution gaps

#### Algorithm Details

**Algorithm: Best-of-N Weighted Selection**
```
Input: Samples S = {s_1, ..., s_N}, verifier V
Output: Best answer

1: answer_groups ‚Üê GroupByFinalAnswer(S)
2: for each (answer, group) ‚àà answer_groups do
3:     score[answer] ‚Üê Œ£ V.Score(s) for s ‚àà group
4: end for
5: return argmax_{answer} score[answer]
```

**Algorithm: Beam Search with PRM**
```
Input: Model M, PRM V, prompt q, budget N, beam width M_w, max_steps
Output: Best solution

1: beams ‚Üê M.SampleStep(q, n=N)
2: for step ‚Üê 1 to max_steps do
3:     if AllComplete(beams) then break
4:     
5:     // Score each beam's current step
6:     for each b ‚àà beams do
7:         score[b] ‚Üê V.ScoreStep(b)
8:     end for
9:     
10:    // Keep top N/M_w beams
11:    top_beams ‚Üê SelectTopK(beams, score, k=N/M_w)
12:    
13:    // Expand each by M_w branches
14:    beams ‚Üê []
15:    for each b ‚àà top_beams do
16:        branches ‚Üê M.ContinueStep(b, n=M_w)
17:        beams ‚Üê beams ‚à™ branches
18:    end for
19: end for
20: return BestOfNWeighted(beams, V)
```

**Algorithm: Lookahead Search**
```
Input: Model M, PRM V, prompt q, budget N, beam width M_w, lookahead k
Output: Best solution
Note: Total cost = N √ó (k+1) generations

1: beams ‚Üê M.SampleStep(q, n=N)
2: for step ‚Üê 1 to max_steps do
3:     if AllComplete(beams) then break
4:     
5:     // For each beam, rollout k steps ahead
6:     for each b ‚àà beams do
7:         rollout ‚Üê M.Rollout(b, k_steps=k, temperature=0)
8:         score[b] ‚Üê V.ScoreStep(rollout[k])  // Score end of rollout
9:     end for
10:    
11:    // Select and expand as in beam search
12:    top_beams ‚Üê SelectTopK(beams, score, k=N/M_w)
13:    beams ‚Üê []
14:    for each b ‚àà top_beams do
15:        beams ‚Üê beams ‚à™ M.ContinueStep(b, n=M_w)
16:    end for
17: end for
18: return BestOfNWeighted(beams, V)
```

### Revision Model Training

**Data Generation Pipeline:**

**Algorithm: Generate Revision Training Trajectories**
```
Input: Model M, training questions Q, n_samples = 64
Output: Training trajectories T

1: T ‚Üê []
2: for each q ‚àà Q do
3:     // Sample multiple attempts
4:     samples ‚Üê M.Generate(q, n=n_samples, temperature=0.8)
5:     
6:     // Separate by correctness
7:     correct ‚Üê {s ‚àà samples : IsCorrect(s, q)}
8:     incorrect ‚Üê {s ‚àà samples : ¬¨IsCorrect(s, q)}
9:     
10:    if correct = ‚àÖ then continue
11:    
12:    for each c ‚àà correct do
13:        // Random trajectory length
14:        k ‚Üê Uniform({0, 1, 2, 3, 4})
15:        
16:        if k = 0 then
17:            trajectory ‚Üê [c]
18:        else
19:            // Find most similar incorrect answer
20:            distances ‚Üê [EditDistance(inc, c) for inc ‚àà incorrect]
21:            last_inc ‚Üê incorrect[ArgMin(distances)]
22:            
23:            // Sample other incorrect answers randomly
24:            others ‚Üê RandomSample(incorrect \ {last_inc}, min(k-1, |incorrect|-1))
25:            trajectory ‚Üê [others..., last_inc, c]
26:        end if
27:        
28:        T ‚Üê T ‚à™ {(q, trajectory)}
29:    end for
30: end for
31: return T
```

**Training Configuration:**
```
Optimizer: AdamW
  - learning_rate: 1e-5
  - batch_size: 128
  - dropout: 0.0
  - betas: (0.9, 0.95)

Early Stopping: 
  - Select checkpoint slightly after validation loss increases
  - On-policy evaluation needed (validation set becomes off-policy)
```

---

## üìä Results Summary

### Practical Decision Guide

**Step 1: Estimate Question Difficulty**
```python
def estimate_difficulty_fast(question, model, verifier, n_samples=32):
    """
    Quick difficulty assessment (production-friendly)
    Trade compute now for better allocation later
    """
    samples = model.generate(question, n=n_samples)
    avg_score = mean([verifier.score(s) for s in samples])
    
    # Map to difficulty bin
    if avg_score > 0.6:    return "EASY"
    elif avg_score > 0.4:  return "MEDIUM"  
    elif avg_score > 0.2:  return "HARD"
    else:                  return "VERY_HARD"
```

**Step 2: Select Strategy Based on Difficulty + Budget**

```python
def select_strategy(difficulty, total_budget, method='revisions'):
    """
    Compute-optimal strategy selection
    """
    if method == 'revisions':
        strategies = {
            'EASY': {
                'seq': total_budget,      # Pure sequential
                'par': 1,
                'expected_gain': '+28%'
            },
            'MEDIUM': {
                'seq': total_budget // 4,  # 4:1 ratio
                'par': 4,
                'expected_gain': '+12-17%'
            },
            'HARD': {
                'seq': total_budget // 16, # 1:4 ratio
                'par': 16,
                'expected_gain': '+5%'
            },
            'VERY_HARD': {
                'seq': 1,                  # Pure parallel
                'par': total_budget,
                'expected_gain': 'Minimal, consider larger model'
            }
        }
    
    elif method == 'prm_search':
        if total_budget < 32:
            # Low budget: beam search wins
            strategies = {
                'EASY': 'best_of_n',      # Don't over-optimize
                'MEDIUM': 'beam_search',
                'HARD': 'beam_search',
                'VERY_HARD': 'beam_search'
            }
        else:
            # High budget: be careful with beam on easy problems
            strategies = {
                'EASY': 'best_of_n',      # Over-optimization risk
                'MEDIUM': 'beam_search',   # Still beneficial
                'HARD': 'best_of_n',       # Both similar
                'VERY_HARD': 'best_of_n'   # Both struggle
            }
    
    return strategies[difficulty]
```

### 1. Compute-Optimal Revisions

**Setup:** PaLM 2-S* revision model on MATH benchmark

| Generation Budget | Parallel (baseline) | Compute-Optimal | Improvement |
|-------------------|--------------------:|----------------:|------------:|
| 16 | 28.2% | 31.8% | +12.8% |
| 32 | 32.4% | 36.7% | +13.3% |
| 64 | 35.8% | 40.5% | +13.1% |
| 128 | 38.1% | 43.2% | +13.4% |

**Key Observation:** Compute-optimal can match parallel baseline at 64 samples using only ~16 samples (4√ó efficiency).

### 2. Compute-Optimal PRM Search

**Setup:** Beam search + lookahead vs. best-of-N

| Generation Budget | Best-of-N | Compute-Optimal | Improvement |
|-------------------|----------:|----------------:|------------:|
| 4 | 18.6% | 21.2% | +14.0% |
| 16 | 25.4% | 28.1% | +10.6% |
| 64 | 31.8% | 34.3% | +7.9% |
| 256 | 35.2% | 37.4% | +6.3% |

**Key Observation:** At lower budgets, beam search significantly outperforms best-of-N. Gains diminish at higher budgets due to PRM over-optimization.

### 3. Difficulty-Dependent Performance

**Understanding the Numbers**: Why performance varies so much

#### Revisions @ 128 generations (Sequential:Parallel ratio)

| Difficulty | Base Pass@1 | Optimal Ratio | Accuracy | Gain | Intuition |
|------------|-------------|---------------|----------|------|-----------|
| **Level 1** (Easy) | 62% | 128:1 (pure seq) | 78.4% | +26.5% | Model makes small mistakes, revisions fix them |
| **Level 2** | 45% | 32:4 | 65.2% | +44.9% | Mostly on track, occasional wrong approach |
| **Level 3** | 28% | 16:8 | 48.1% | +71.8% | 50/50 need refinement vs. new approach |
| **Level 4** | 15% | 8:16 | 32.7% | +118% | Usually wrong approach, need exploration |
| **Level 5** (Hard) | 8% | 4:32 | 18.3% | +129% | Almost always wrong, but still limited help |

**Key Observation**: Absolute gains decrease with difficulty, but *relative* gains can be large. However, very hard problems still have poor absolute performance.

#### PRM Search by Difficulty @ N=64

| Difficulty | Best-of-N | Beam (M=4) | Winner | Gap | Why |
|------------|-----------|------------|--------|-----|-----|
| **Level 1** | 68.2% | 65.4% | Best-of-N | +2.8% | Beam over-optimizes verifier |
| **Level 2** | 52.4% | 54.1% | Beam | +1.7% | Slight benefit from search |
| **Level 3** | 36.8% | 38.6% | Beam | +1.8% | Search helps find solutions |
| **Level 4** | 22.9% | 24.1% | Beam | +1.2% | Both struggle, beam slightly better |
| **Level 5** | 8.1% | 8.3% | ~ | +0.2% | Neither method helps much |

**The Over-Optimization Pattern**:
```
Easy Problems + High Compute ‚Üí Over-optimization Risk
  ‚îú‚îÄ Verifier is accurate (model usually right)
  ‚îú‚îÄ Strong optimizer finds edge cases
  ‚îî‚îÄ Result: Worse than simple sampling

Hard Problems + High Compute ‚Üí Search Beneficial  
  ‚îú‚îÄ Verifier less accurate BUT model rarely right anyway
  ‚îú‚îÄ Search helps find rare good solutions
  ‚îî‚îÄ Result: Consistent improvements
```

### 4. Test-Time vs. Pretraining FLOPs (Detailed Breakdown)

**Experimental Setup**: Compare PaLM 2-S* (smaller) + test-time compute vs. 14√ó larger model (greedy decoding)

**The Three Scenarios Based on R = D_inference / D_pretrain:**

#### Scenario 1: R << 1 (e.g., R = 0.16) - Self-Improvement Use Case
*Generate training data: low inference, high pretraining*

**Equivalent Test-Time Budget**: ~69 samples to match 14√ó model FLOPs

| Method | Difficulty 1 | Difficulty 2 | Difficulty 3 | Difficulty 4 | Difficulty 5 |
|--------|-------------|-------------|-------------|-------------|-------------|
| **Revisions (small + test)** | 78.4% | 65.2% | 48.1% | 32.7% | 18.3% |
| **14√ó model (greedy)** | 64.5% | 58.8% | 36.3% | 44.5% | 28.4% |
| **Winner** | üü¢ Test-time (+21.6%) | üü¢ Test-time (+10.9%) | üü¢ Test-time (+32.5%) | üî¥ Pretraining (-36.1%) | üî¥ Pretraining (-35.6%) |

**Interpretation**: For easy/medium problems, test-time wins big. Hard problems need the larger model's base capability.

#### Scenario 2: R ‚âà 1 (e.g., R = 0.79) - Balanced Usage
*Equal inference and pretraining loads*

**Equivalent Test-Time Budget**: ~560 samples

| Method | Difficulty 1 | Difficulty 2 | Difficulty 3 | Difficulty 4 | Difficulty 5 |
|--------|-------------|-------------|-------------|-------------|-------------|
| **Revisions** | 82.1% | 68.5% | 51.2% | 34.8% | 19.1% |
| **14√ó model** | 64.5% | 58.8% | 36.3% | 44.5% | 28.4% |
| **Winner** | üü¢ Test-time (+27.3%) | üü¢ Test-time (+16.5%) | üü¢ Test-time (+41.0%) | üî¥ Pretraining (-21.8%) | üî¥ Pretraining (-32.7%) |

**Interpretation**: Pattern similar to R << 1, but with more test-time budget available.

#### Scenario 3: R >> 1 (e.g., R = 22) - Production Deployment
*Millions of inference queries per pretraining token*

**Equivalent Test-Time Budget**: ~5,474 samples (extremely high)

| Method | Difficulty 1 | Difficulty 2 | Difficulty 3 | Difficulty 4 | Difficulty 5 |
|--------|-------------|-------------|-------------|-------------|-------------|
| **Revisions** | 83.8% | 69.2% | 52.1% | 35.2% | 19.4% |
| **14√ó model** | 64.5% | 58.8% | 36.3% | 44.5% | 28.4% |
| **Winner** | üü¢ Test-time (+29.9%) | üü° Test-time (+17.7%) | üü¢ Test-time (+43.5%) | üî¥ Pretraining (-20.9%) | üî¥ Pretraining (-31.7%) |

**BUT**: In production, cost per query matters more than total FLOPs:
- **5,474 samples per query** is impractical
- **Latency**: Users can't wait for thousands of generations  
- **Real-world verdict**: Pretraining wins for production at scale

**PRM Search Shows Similar Pattern**:

For R = 0.16:
- Easy: Test-time +2.2%, Medium: +19.1%, Hard: -35.3% to -52.9%

For R >> 1:  
- Easy: Test-time +2.0%, Medium: +0%, Hard: -30.6% to -52.9%

### Summary Decision Matrix

```
                Easy Questions    Medium Questions    Hard Questions
                (Pass@1 > 60%)   (Pass@1: 30-60%)   (Pass@1 < 30%)
R << 1          Test-time ‚úì      Test-time ‚úì        Pretraining ‚úì
(Self-improve)  (+20-30%)        (+10-20%)          (-25-40%)

R ‚âà 1           Test-time ‚úì      Test-time ‚úì        Pretraining ‚úì  
(Balanced)      (+25-30%)        (+15-20%)          (-20-35%)

R >> 1          Pretraining*     Pretraining*       Pretraining ‚úì
(Production)    (*Latency)       (*Latency)         (-30-50%)
```

**The Fundamental Insight**:
1. **Test-time compute is NOT a universal replacement for pretraining**
2. **It works within model's capability range** (easy/medium problems)
3. **It fails beyond model's capabilities** (very hard problems)
4. **Production constraints** (latency, cost per query) favor pretraining even when FLOPs favor test-time

---

## üèóÔ∏è Architecture Components

### 1. Process Reward Model (PRM)

**Architecture:**
```
Base LM ‚Üí [STEP_1] ‚Üí [STEP_2] ‚Üí ... ‚Üí [STEP_N]
          ‚Üì         ‚Üì                ‚Üì
        r‚ÇÅ ‚àà[0,1] r‚ÇÇ ‚àà[0,1]  ...  r‚Çô ‚àà[0,1]
```

**Training Objective:**
```
Minimize: L = -Œ£·µ¢ [y·µ¢ log(rÃÇ·µ¢) + (1-y·µ¢) log(1-rÃÇ·µ¢)]

where:
  rÃÇ·µ¢ = predicted step correctness (model output)
  y·µ¢ = soft label from Monte Carlo rollouts
```

**Algorithm: PRM Training Data Generation**
```
Input: Base model M, questions Q, n_samples = 16, n_rollouts = 16
Output: Training dataset D

1: D ‚Üê []
2: for each q ‚àà Q do
3:     solutions ‚Üê M.Generate(q, n=n_samples, few_shot_prompt)
4:     
5:     for each sol ‚àà solutions do
6:         if ¬¨IsValidFormat(sol) then continue
7:         
8:         steps ‚Üê ParseIntoSteps(sol)  // Split by newlines
9:         for i ‚Üê 1 to |steps| do
10:            // Estimate value via Monte Carlo rollouts
11:            prefix ‚Üê steps[1:i]
12:            successes ‚Üê 0
13:            for j ‚Üê 1 to n_rollouts do
14:                completion ‚Üê M.Complete(q, prefix)
15:                if IsCorrect(completion, q) then
16:                    successes ‚Üê successes + 1
17:                end if
18:            end for
19:            value ‚Üê successes / n_rollouts  // Soft label
20:            D ‚Üê D ‚à™ {(q, prefix, steps[i], value)}
21:        end for
22:    end for
23: end for
24: return D
```

**Aggregation for final score:** Use last step prediction (outperforms min/product)
```
FinalScore(solution) = rÃÇ‚Çô  where n = |steps|
```

### 2. Revision Model

**Architecture:**
```
Input: [Question, Prev_Ans_1, ..., Prev_Ans_k, New_Answer]
                                                ‚Üë
                                             (generate this)

Context Window: Last k=4 previous answers + question
```

**Generation Process:**
```
For i = 1 to n_revisions:
  context_i = [q, ans_{max(0,i-k)}, ..., ans_{i-1}]
  ans_i ~ M_rev(¬∑ | context_i)
```

**Key Capabilities:**
- Identifies errors in previous attempts
- Makes targeted corrections
- Generalizes beyond training (4 revisions) ‚Üí tested up to 64

**Algorithm: Revision Chain Generation**
```
Input: Revision model M_rev, question q, n_revisions n, max_context k=4
Output: Chain of answers

1: answers ‚Üê []
2: for i ‚Üê 1 to n do
3:     // Build context from last k answers
4:     start ‚Üê max(0, i - k)
5:     context ‚Üê [q, answers[start], ..., answers[i-1]]
6:     
7:     // Generate next revision
8:     ans_i ‚Üê M_rev.Generate(context)
9:     answers.Append(ans_i)
10: end for
11: return answers
```

### 3. Answer Selection Strategies

**Algorithm: Hierarchical Selection (for revisions)**
```
Input: Chains C = {c‚ÇÅ, ..., c‚Çò} where c‚±º = [ans¬π‚±º, ..., ans‚Åø‚±º], verifier V
Output: Best answer

// Step 1: Within-chain selection
1: best_per_chain ‚Üê []
2: for j ‚Üê 1 to m do
3:     scores ‚Üê [V.Score(ans‚Å±‚±º) for i ‚àà {1,...,n}]
4:     best ‚Üê WeightedBestOfN(c‚±º, scores)
5:     best_per_chain.Append(best)
6: end for

// Step 2: Cross-chain selection
7: final_scores ‚Üê [V.Score(ans) for ans ‚àà best_per_chain]
8: return WeightedBestOfN(best_per_chain, final_scores)
```

**Algorithm: Weighted Best-of-N**
```
Input: Answers A = {a‚ÇÅ, ..., a‚Çô}, scores S = {s‚ÇÅ, ..., s‚Çô}
Output: Best answer

1: answer_to_score ‚Üê {}  // Map: final_answer ‚Üí total_score
2: for i ‚Üê 1 to n do
3:     final_ans ‚Üê ExtractFinalAnswer(a·µ¢)
4:     if final_ans ‚àâ answer_to_score then
5:         answer_to_score[final_ans] ‚Üê 0
6:     end if
7:     answer_to_score[final_ans] ‚Üê answer_to_score[final_ans] + s·µ¢
8: end for
9: return argmax_{ans} answer_to_score[ans]
```

**Algorithm: Majority Voting (simpler, for flat lists)**
```
Input: All answers A = {a‚ÇÅ, ..., a‚Çô}
Output: Most common answer

1: final_answers ‚Üê [ExtractFinalAnswer(a) for a ‚àà A]
2: counts ‚Üê CountOccurrences(final_answers)
3: return argmax_{ans} counts[ans]

Note: Works better than hierarchical for small chain lengths
```

---

## üß™ Experimental Setup

### Dataset
- **MATH benchmark**: 12,000 train + 500 test
- High-school competition math problems
- Multiple difficulty levels
- Chosen because: requires inference over existing knowledge (not new knowledge)

### Base Model
- **PaLM 2-S* (Codey)**
- Non-trivial but not saturated performance on MATH
- Representative of contemporary LLM capabilities
- Requires capability-specific finetuning for revision/verification

### Evaluation Protocol

**Two-Fold Cross-Validation:**
```python
def compute_optimal_evaluation(test_set, strategies):
    """
    Avoid overfitting to test set
    """
    # Split into two folds
    fold1, fold2 = split(test_set, ratio=0.5)
    
    # Fold 1: select strategy, evaluate on fold 2
    best_strategy_1 = select_best(strategies, validate_on=fold1)
    score_2 = evaluate(best_strategy_1, test_on=fold2)
    
    # Fold 2: select strategy, evaluate on fold 1
    best_strategy_2 = select_best(strategies, validate_on=fold2)
    score_1 = evaluate(best_strategy_2, test_on=fold1)
    
    # Average results
    return (score_1 + score_2) / 2
```

### Metrics

**Pass@1 per step:**
```python
def compute_pass_at_1(model, questions, step_idx):
    """
    Probability of getting correct answer at step i
    """
    correct = 0
    for q in questions:
        answer_chain = model.generate_revisions(q, n=step_idx+1)
        if is_correct(answer_chain[step_idx], q):
            correct += 1
    return correct / len(questions)
```

**Generation Budget:**
- Count: number of independent LM forward passes
- For lookahead: multiply by (k+1) due to rollouts
- For revisions: N_parallel √ó N_sequential

---

## üíª Implementation Details

### Training Configurations

**PRM Training:**
```python
config = {
    'optimizer': 'AdamW',
    'learning_rate': 3e-5,
    'batch_size': 128,
    'dropout': 0.05,
    'betas': (0.9, 0.95),
    'early_stopping': 'val_loss',
    
    'data': {
        'samples_per_question': 16,
        'rollouts_per_step': 16,
        'filter_invalid': True,  # Remove unparseable answers
    }
}
```

**Revision Model Training:**
```python
config = {
    'optimizer': 'AdamW',
    'learning_rate': 1e-5,
    'batch_size': 128,
    'dropout': 0.0,
    'betas': (0.9, 0.95),
    'early_stopping': 'slightly_after_overfit',  # Important!
    
    'data': {
        'samples_per_question': 64,
        'max_context_length': 4,
        'trajectory_lengths': [0, 1, 2, 3, 4],  # Uniform sampling
        'use_edit_distance': True,  # For pairing incorrect‚Üícorrect
    }
}
```

**ORM for Revisions (separate from PRM):**
```python
# Train ORM specifically on revision model outputs
config = {
    'base': 'PRM_config',  # Same as PRM
    'include_context': True,  # Key difference: revision history in context
}
```

### Prompting Strategy

**Few-Shot Prompt:**
- 4-shot examples from PRM800k phase 1
- GPT-4 generated step-by-step solutions
- Forces model to output line-separated steps
- Enables step-wise PRM scoring

```python
def create_prompt(question, examples):
    """
    4-shot prompt with step-by-step format
    """
    prompt = ""
    for ex in examples[:4]:  # 4 random correct examples
        prompt += f"Question: {ex.question}\n"
        prompt += "Solution:\n"
        for step in ex.steps:
            prompt += f"{step}\n"
        prompt += f"Final Answer: {ex.answer}\n\n"
    
    prompt += f"Question: {question}\n"
    prompt += "Solution:\n"
    return prompt
```

### Cost Analysis

**FLOPs Calculations:**
```
Pretraining FLOPs: X = 6 √ó N √ó D_pretrain
Inference FLOPs:   Y = 2 √ó N √ó D_inference

where N = model parameters, D = tokens

Scaling model by factor M:
  Total FLOPs = M √ó (X + Y) = M √ó X + M √ó Y

Matching with test-time compute:
  Test-time FLOPs = (M + 3R(M-1)) √ó Y
  where R = D_inference / D_pretrain
```

```python
def compute_flops_equivalent(M, R, base_params, D_pretrain, D_inference):
    """
    How many test-time samples match pretraining compute scaling?
    
    M: model scale multiplier (e.g., 14x)
    R: inference to pretrain token ratio
    """
    # Larger model cost
    pretrain_flops = 6 * (M * base_params) * D_pretrain
    inference_flops_base = 2 * (M * base_params) * D_inference
    total_large_model = pretrain_flops + inference_flops_base
    
    # Smaller model cost
    pretrain_flops_small = 6 * base_params * D_pretrain
    inference_budget = total_large_model - pretrain_flops_small
    
    # How many generations can we afford?
    flops_per_generation = 2 * base_params
    n_generations = inference_budget / flops_per_generation
    
    return int(n_generations)
```

---

## üìà Additional Findings

### 1. PRM Aggregation Comparison

Tested three methods for aggregating step scores:

| Method | Description | Performance |
|--------|-------------|-------------|
| **Last** ‚úÖ | Use only final step score | Best |
| Min | Take minimum across all steps | Worse |
| Product | Multiply all step probabilities | Worse |

**Hypothesis:** Soft MC labels (vs. binary labels) change optimal aggregation strategy.

### 2. PRM vs. ORM

PRM consistently outperforms ORM, especially at high sample counts:

| Samples | ORM Accuracy | PRM Accuracy | Gap |
|---------|--------------|--------------|-----|
| 2 | 12.4% | 13.1% | +0.7% |
| 8 | 18.6% | 21.2% | +2.6% |
| 32 | 24.3% | 28.7% | +4.4% |
| 128 | 28.1% | 34.2% | +6.1% |

### 3. Beam Search Over-Optimization

Beam search shows different patterns by difficulty:

**Easy Problems (Level 1-2):**
- Low budget: Beam search >> Best-of-N
- High budget: Beam search < Best-of-N (over-optimization)

**Hard Problems (Level 3-4):**
- All budgets: Beam search ‚â• Best-of-N
- Consistent improvements with more compute

**Very Hard (Level 5):**
- All methods struggle
- Little benefit from any test-time strategy

### 4. Sequential vs. Parallel Tradeoff

**Revision model pass@1 progression:**
```
Step 0 (initial): 17.2%
Step 1:           18.4% (+1.2%)
Step 2:           19.3% (+0.9%)
Step 4:           20.8% (+1.5%)
Step 8:           22.1% (+1.3%)
Step 16:          23.5% (+1.4%)
...continues improving...
```

**Optimal ratios (N=128):**
```
Difficulty 1:  128:1  (pure sequential)
Difficulty 2:   32:4
Difficulty 3:   16:8  (balanced)
Difficulty 4:    8:16
Difficulty 5:    4:32 (mostly parallel)
```

---

## üöÄ Future Directions

The paper identifies several key areas for future work:

### 1. Improved Test-Time Scaling
- Combine PRM search with revisions
- Integrate critique-and-revise methods
- Develop methods that work on very hard problems (level 5)

### 2. Efficient Difficulty Assessment
```python
# Current: expensive (2048 samples)
difficulty = estimate_from_samples(model, question, n=2048)

# Proposed: finetune dedicated predictor
difficulty = difficulty_predictor(question)  # Single forward pass
```

### 3. Exploration-Exploitation Balance
```python
def adaptive_compute_allocation(question, budget):
    """
    Balance between:
    - Estimating difficulty (exploration)
    - Solving with compute-optimal strategy (exploitation)
    """
    exploration_budget = budget * alpha
    exploitation_budget = budget * (1 - alpha)
    
    difficulty = estimate_difficulty(question, exploration_budget)
    strategy = select_strategy(difficulty)
    answer = solve(question, strategy, exploitation_budget)
    
    return answer
```

### 4. Iterative Self-Improvement Loop
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Base LM                                  ‚îÇ
‚îÇ    ‚Üì                                      ‚îÇ
‚îÇ  Apply test-time compute                 ‚îÇ
‚îÇ    ‚Üì                                      ‚îÇ
‚îÇ  Generate improved solutions             ‚îÇ
‚îÇ    ‚Üì                                      ‚îÇ
‚îÇ  Distill back into base LM              ‚îÇ
‚îÇ    ‚Üì                                      ‚îÇ
‚îÇ  Repeat...                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö Citation

```bibtex
@article{snell2024scaling,
  title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}
```

---

## ‚ùì Common Questions & Troubleshooting

### Q: Why does beam search hurt performance on easy problems?

**A: Over-optimization of the verifier.**

When the base model already generates good answers (easy problems):
- Verifier is mostly correct in its predictions
- Beam search exploits edge cases where verifier is wrong
- Result: Finds adversarial examples that fool verifier but are actually wrong

**Solution**: Use best-of-N on easy problems, beam search only on medium/hard.

### Q: My revision model makes correct answers worse. Why?

**A: Distribution shift at inference time.**

The model was trained on sequences like: `[wrong, wrong, wrong, correct]`

At test time, it sometimes generates: `[wrong, correct, wrong]` ‚Üê Oops!

**Why it happens**:
- Training: Only incorrect answers in context
- Inference: Correct answer appears in context
- Model tries to "fix" what isn't broken

**Solutions**:
1. Use verifier-based selection (pick best from chain)
2. Majority voting across all revisions
3. Train with mixed trajectories (include correct ‚Üí correct)

**The paper's approach**: Hierarchical selection catches this ~62% of the time.

### Q: When should I use lookahead search?

**A: Almost never in practice.**

Lookahead is theoretically superior but:
- Cost: N √ó (k+1) generations
- At k=3: Uses 4√ó the compute of beam search
- Performance: Barely better than beam search (if at all)

**Better approach**: Use those extra samples for more beams or best-of-N.

### Q: How do I know if my verifier is good enough?

**A: Check agreement with ground truth:**

**Algorithm: Evaluate Verifier Quality**
```
Input: Verifier V, model M, test questions Q
Output: Quality metrics

1: Initialize: accuracy ‚Üê 0, FP_rate ‚Üê 0, FN_rate ‚Üê 0, overopt_risk ‚Üê 0
2: for each q ‚àà Q do
3:     samples ‚Üê M.Generate(q, n=32)
4:     verifier_scores ‚Üê [V.Score(s) for s ‚àà samples]
5:     ground_truth ‚Üê [IsCorrect(s, q) for s ‚àà samples]
6:     
7:     // Agreement rate
8:     predictions ‚Üê [score > 0.5 for score ‚àà verifier_scores]
9:     accuracy ‚Üê accuracy + Accuracy(predictions, ground_truth)
10:    
11:    // False positives (wrong but high score)
12:    FP ‚Üê |{i : predictions[i] ‚àß ¬¨ground_truth[i]}|
13:    FP_rate ‚Üê FP_rate + FP / |samples|
14:    
15:    // False negatives (correct but low score)
16:    FN ‚Üê |{i : ¬¨predictions[i] ‚àß ground_truth[i]}|
17:    FN_rate ‚Üê FN_rate + FN / |samples|
18:    
19:    // Over-optimization risk on easy problems
20:    pass_rate ‚Üê Œ£ ground_truth / |samples|
21:    if pass_rate > 0.6 then  // Easy problem
22:        overopt_risk ‚Üê overopt_risk + StdDev(verifier_scores)
23:    end if
24: end for
25: 
26: // Average across questions
27: accuracy ‚Üê accuracy / |Q|
28: FP_rate ‚Üê FP_rate / |Q|
29: FN_rate ‚Üê FN_rate / |Q|
30: overopt_risk ‚Üê overopt_risk / |Q|
31:
32: return {accuracy, FP_rate, FN_rate, overopt_risk}
```

**Acceptable thresholds**:
```
Accuracy:         >75% (higher is better)
False Positive:   <20%
False Negative:   <30%  
Over-opt Risk:    <0.25 (std dev on easy problems)
```

**If verifier is poor**: Stick to majority voting or simple best-of-N.

### Q: How much does difficulty estimation cost?

**A: 16-32 samples (small overhead, can be reused).**

**Breakdown**:
```
Difficulty estimation: 16 samples
Actual solving: 64 samples
Total: 80 samples

Without difficulty: 64 samples (wrong strategy)
With difficulty: 80 samples (right strategy)

Net result: 25% overhead, but 4√ó better efficiency
‚Üí Total gain: ~3√ó efficiency
```

**Optimization**: Reuse estimation samples in solving phase.

### Q: What if I can't finetune a revision model?

**A: Use prompt-based revisions:**

**Algorithm: Simple Revision via Prompting (No Finetuning)**
```
Input: Question q, previous attempt prev_ans, base model M
Output: Revised answer

1: prompt ‚Üê ConstructPrompt(q, prev_ans)
2: revised_ans ‚Üê M.Generate(prompt)
3: return revised_ans

Function ConstructPrompt(q, prev_ans):
    return """
    Question: {q}
    
    Previous attempt:
    {prev_ans}
    
    Please review the previous attempt and identify any errors.
    Then provide a corrected solution.
    
    Review:
    """
```

**Trade-offs**:
- ‚úÖ No finetuning required
- ‚úÖ Works with any model
- ‚ùå Less effective than finetuned revision model
- ‚ùå Higher token cost (longer prompts)

---

### Q: My results don't match the paper. What's wrong?

**Common issues**:

1. **Verifier quality**: Paper uses specialized PRM with MC rollouts
   - Your verifier might be weaker
   - Try: Train better verifier or use majority voting

2. **Model capability**: Paper uses PaLM 2-S* (non-trivial but not saturated)
   - If your model is too weak (pass@1 < 5%): Nothing helps much
   - If your model is too strong (pass@1 > 80%): Less room for improvement

3. **Dataset difficulty**: MATH benchmark has specific properties
   - Right difficulty range (not too easy/hard)
   - Pure reasoning (not knowledge)
   - Your domain might differ

4. **Search hyperparameters**: Beam width, temperature, etc.
   - Paper uses M=4 for beam search
   - Try: Grid search on validation set

---

### Q: How does this compare to recent work (o1, o3)?

**A: Different approaches, complementary insights.**

**This paper (2024-08)**:
- Focus: Systematic analysis of test-time scaling
- Methods: Revisions + PRM search
- Key insight: Compute-optimal depends on difficulty

**OpenAI o1/o3 (2024-12+)**:
- Focus: Training models to "think" via RL
- Methods: RL on chain-of-thought, integrated verifiers
- Key insight: Models can learn when/how to use compute

**Relationship**:
- This paper: "How to allocate test-time compute given a model"
- o1/o3: "How to train models to use test-time compute natively"
- Future: Combine both‚Äîtrain models for test-time + optimal allocation

---

**A: Yes! (Future work)**

The paper tests them separately, but you could:

**Algorithm: Hybrid Revision + Search**
```
Input: Question q, revision model M_rev, PRM V, budget N
Output: Best answer

// Phase 1: Generate diverse candidates with revisions
1: chains ‚Üê []
2: for i ‚Üê 1 to N/8 do
3:     chain ‚Üê M_rev.GenerateRevisionChain(q, n_revisions=4)
4:     chains.Append(chain)
5: end for

// Phase 2: Use PRM to select promising directions
6: all_candidates ‚Üê Flatten(chains)
7: scores ‚Üê [V.Score(c) for c ‚àà all_candidates]
8: top_k ‚Üê SelectTopK(all_candidates, scores, k=N/4)

// Phase 3: Beam search from top candidates
9: final_candidates ‚Üê []
10: for each candidate ‚àà top_k do
11:    beams ‚Üê BeamSearchFrom(candidate, budget=N/(4√ó|top_k|))
12:    final_candidates.Extend(beams)
13: end for

14: return BestOfNWeighted(final_candidates, V)
```

**Expected benefit**: Best of both worlds (exploration + exploitation).

### Q: What about other domains (code, creative writing)?

**A: Core principles transfer, details differ.**

**Code generation**:
- Easy = fixing syntax errors ‚Üí Revisions work great
- Hard = algorithm design ‚Üí Need exploration
- Verifier = test suite execution (very reliable)
- **Insight**: Better verifier makes search more effective

**Creative writing**:
- Difficulty estimation harder (subjective quality)
- Verifier harder (no ground truth)
- Revisions likely more useful (refinement-heavy)
- **Insight**: Might need human-in-the-loop

**Scientific reasoning**:
- Similar to MATH (pure reasoning)
- Verifier could be literature consistency check
- **Insight**: Likely similar patterns to MATH results

---

## üîó Resources

- **Paper:** [arXiv:2408.03314](https://arxiv.org/abs/2408.03314)
- **MATH Dataset:** [GitHub](https://github.com/hendrycks/math)
- **PRM800K:** Released by OpenAI (Lightman et al., 2023)

---

## üõ†Ô∏è Practical Implementation Guide

### When Should You Use Test-Time Compute?

**Use Test-Time Compute When:**
1. ‚úÖ Working on problems within model's capability (pass@1 > 10%)
2. ‚úÖ Low inference-to-pretraining ratio (R << 1)
3. ‚úÖ Can afford latency (not real-time applications)
4. ‚úÖ Have good verifier/reward model
5. ‚úÖ Problems vary in difficulty (benefit from adaptive allocation)

**Use Larger Pretrained Model When:**
1. ‚ùå Problems are beyond base model (pass@1 < 5%)
2. ‚ùå High inference load (R >> 1, production scale)
3. ‚ùå Need low latency (<1s response time)
4. ‚ùå Verifier quality is poor
5. ‚ùå Problems are uniformly hard

### Main Algorithm: Adaptive Test-Time Solver

**Algorithm: Compute-Optimal Adaptive Solver**
```
Input: Question q, base model M, revision model M_rev, PRM V, budget N, config
Output: Best answer

// Step 1: Quick difficulty assessment
1: difficulty, confidence ‚Üê EstimateDifficulty(q, M, V, n_samples=16)

// Step 2: Select strategy
2: strategy ‚Üê SelectStrategy(difficulty, N, config)

// Step 3: Execute appropriate method
3: if strategy.method = "revisions" then
4:     answer ‚Üê SolveWithRevisions(q, M_rev, V, strategy.n_seq, strategy.n_par)
5: else if strategy.method = "prm_search" then
6:     answer ‚Üê SolveWithPRM(q, M, V, strategy.algorithm, N)
7: else
8:     answer ‚Üê M.Generate(q)  // Fallback
9: end if
10: return answer
```

**Algorithm: Fast Difficulty Estimation**
```
Input: Question q, model M, verifier V, n_samples = 16
Output: Difficulty level, confidence

1: samples ‚Üê M.Generate(q, n=n_samples)
2: scores ‚Üê [V.Score(s) for s ‚àà samples]
3: avg_score ‚Üê Mean(scores)
4: std_score ‚Üê StdDev(scores)

5: // Map to difficulty category
6: if avg_score > 0.6 then
7:     difficulty ‚Üê "EASY"
8: else if avg_score > 0.35 then
9:     difficulty ‚Üê "MEDIUM"
10: else if avg_score > 0.15 then
11:    difficulty ‚Üê "HARD"
12: else
13:    difficulty ‚Üê "VERY_HARD"
14: end if

15: // Confidence from score variance
16: if avg_score > 0 then
17:    confidence ‚Üê 1.0 - min(std_score / avg_score, 1.0)
18: else
19:    confidence ‚Üê 0.0
20: end if

21: return difficulty, confidence
```

**Algorithm: Strategy Selection with Constraints**
```
Input: Difficulty d, budget N, config (max_samples, latency_budget)
Output: Strategy parameters

1: N ‚Üê min(N, config.max_samples)  // Enforce cost constraint

2: // Define strategies by difficulty
3: if d = "EASY" then
4:     strategy ‚Üê {method: "revisions", n_seq: min(N, 32), n_par: max(1, N/32)}
5:     expected_time ‚Üê N √ó 50  // 50ms per generation
6: else if d = "MEDIUM" then
7:     strategy ‚Üê {method: "revisions", n_seq: N/4, n_par: 4}
8:     expected_time ‚Üê N √ó 50
9: else if d = "HARD" then
10:    if N < 64 then
11:        algorithm ‚Üê "beam_search"
12:    else
13:        algorithm ‚Üê "best_of_n"
14:    end if
15:    strategy ‚Üê {method: "prm_search", algorithm: algorithm}
16:    expected_time ‚Üê N √ó 60
17: else  // VERY_HARD
18:    strategy ‚Üê {method: "best_of_n"}
19:    expected_time ‚Üê N √ó 55
20: end if

21: // Check latency constraint
22: if expected_time > config.latency_budget then
23:    scale ‚Üê config.latency_budget / expected_time
24:    if "n_seq" ‚àà strategy then
25:        strategy.n_seq ‚Üê max(1, ‚åästrategy.n_seq √ó scale‚åã)
26:        strategy.n_par ‚Üê max(1, ‚åästrategy.n_par √ó scale‚åã)
27:    end if
28: end if

29: return strategy
```

**Algorithm: Solve with Revisions (Hybrid Seq/Par)**
```
Input: Question q, revision model M_rev, verifier V, n_seq, n_par
Output: Best answer

1: all_chains ‚Üê []

2: // Generate n_par chains in parallel
3: for i ‚Üê 1 to n_par do
4:     chain ‚Üê M_rev.GenerateRevisionChain(q, n_revisions=n_seq)
5:     all_chains.Append(chain)
6: end for

7: // Hierarchical selection
8: best_per_chain ‚Üê []
9: for chain ‚àà all_chains do
10:    scores ‚Üê [V.Score(ans) for ans ‚àà chain]
11:    best ‚Üê WeightedBestOfN(chain, scores)
12:    best_per_chain.Append(best)
13: end for

14: // Final selection across chains
15: final_scores ‚Üê [V.Score(ans) for ans ‚àà best_per_chain]
16: return WeightedBestOfN(best_per_chain, final_scores)
```

**Algorithm: Solve with PRM Search**
```
Input: Question q, model M, PRM V, algorithm, budget N
Output: Best answer

1: if algorithm = "beam_search" then
2:     return BeamSearch(M, V, q, N, M_width=4)
3: else  // best_of_n
4:     samples ‚Üê M.Generate(q, n=N)
5:     return BestOfNWeighted(samples, V)
6: end if
```

### Cost-Benefit Analysis Example

**Scenario**: Math tutoring application with 1M queries/day

**Option A: Larger Model (14√ó parameters)**
```
Pretraining Cost: $10M (one-time)
Inference Cost per query: $0.02
Daily Cost: $0.02 √ó 1M = $20K
Annual Cost: $7.3M

Total Year 1: $10M + $7.3M = $17.3M
Latency: 200ms average
```

**Option B: Smaller Model + Test-Time Compute**
```
Pretraining Cost: $0.7M (one-time)
Inference Cost per query: $0.01 √ó avg(32 samples) = $0.32
Daily Cost: $0.32 √ó 1M = $320K
Annual Cost: $116.8M

Total Year 1: $0.7M + $116.8M = $117.5M ‚ùå
Note: Too expensive due to high per-query cost
```

**Option C: Hybrid (Compute-Optimal)**
```
Pretraining Cost: $0.7M (one-time)

Inference Cost (difficulty-based routing):
  Easy (60% of queries):   $0.01 √ó 8 samples  = $0.08/query
  Medium (30% of queries): $0.01 √ó 32 samples = $0.32/query  
  Hard (10% of queries):   Use large model    = $0.20/query

Weighted Average: 0.6 √ó $0.08 + 0.3 √ó $0.32 + 0.1 √ó $0.20 = $0.164/query
Daily Cost: $0.164 √ó 1M = $164K
Annual Cost: $59.9M

Total Year 1: $0.7M + $59.9M = $60.6M ‚úì
Performance: Matches or exceeds Option A on 90% of queries
```

**Verdict**: Hybrid approach optimal for this workload profile.

## üìù Key Takeaways

### For Researchers

1. **Test-time compute is difficulty-dependent** 
   - Easy problems: Sequential revisions (like humans checking their work)
   - Hard problems: Parallel search (like trying different approaches)
   - Very hard: Need better models first

2. **4√ó efficiency gains are achievable**
   - Compute-optimal allocation >> naive best-of-N
   - But requires difficulty estimation (small overhead)

3. **Over-optimization is a real problem**
   - Strong optimizers (beam search) can exploit verifier weaknesses
   - More evident on easy problems where verifier is usually right
   - Suggests need for more robust verifiers

4. **Small model + test-time ‚â† always better than large model**
   - Works within capability range (pass@1 > 10%)
   - Fails beyond capabilities (pass@1 < 5%)
   - Tradeoff depends on R = inference/pretraining ratio

5. **Two complementary mechanisms work together:**
   - Revisions: Improve proposal distribution (local refinement)
   - Search: Optimize against verifier (global exploration)
   - Combining them is future work

### For Practitioners

1. **When to deploy test-time compute:**
   - ‚úÖ Problems within model capability
   - ‚úÖ Low inference load (R << 1)
   - ‚úÖ Can afford 1-5s latency
   - ‚úÖ Have diverse difficulty distribution
   - ‚ùå Avoid for: real-time apps, uniform hard problems, poor verifiers

2. **Implementation priority order:**
   ```
   Level 1: Implement best-of-N with good verifier (baseline)
   Level 2: Add difficulty estimation (16-32 samples)
   Level 3: Add revision model for easy/medium questions
   Level 4: Add beam search for hard questions (low budget)
   Level 5: Implement full compute-optimal strategy
   ```

3. **Production considerations:**
   - **Latency matters more than FLOPs** in user-facing apps
   - **Cost per query** drives decisions at scale
   - **Hybrid approach**: Route by difficulty to different strategies
   - **Monitor over-optimization**: Track verifier agreement with ground truth

4. **Training requirements:**
   - **PRM**: Need diverse on-policy samples, Monte Carlo rollouts work
   - **Revision model**: Need incorrect‚Üícorrect trajectories, edit distance helps
   - **Both**: Capability-specific finetuning required (future models may have built-in)

5. **Quick wins you can implement today:**
   - Best-of-N with any verifier (even simple heuristics)
   - Majority voting (free verifier!)
   - Temperature sampling (cheap exploration)

### For Decision Makers

1. **Strategic implications:**
   - **Shift from "bigger models" to "smarter inference"**
   - Test-time compute enables smaller on-device models
   - But not a silver bullet‚Äîstill need strong base models

2. **Investment tradeoffs:**
   - **Self-improvement pipelines** (R << 1): Invest in test-time compute
   - **Production at scale** (R >> 1): Invest in larger models
   - **Hybrid**: Most realistic for diverse workloads

3. **Future trajectory:**
   - Current work shows even naive methods work
   - As test-time strategies improve: balance shifts toward inference
   - Long-term: **Fewer pretraining FLOPs, more inference FLOPs**

4. **Risk factors:**
   - Verifier quality is critical‚Äîpoor verifier = wasted compute
   - Over-optimization can hurt, not help
   - Very hard problems still need pretraining advances

### The Big Picture

**This paper shows test-time compute is not just "more samples":**

```
Naive Approach:        Sample more ‚Üí sometimes better
Smart Approach:        Estimate difficulty ‚Üí select strategy ‚Üí allocate optimally
Result:                4√ó more efficient, predictable improvements

Future Vision:         
- Models pretrained with test-time compute in mind
- Automatic strategy selection
- Iterative self-improvement loops  
- Shift from "train giant models" to "train smart models + smart inference"
```

**The fundamental insight**: LLMs should think adaptively, just like humans do. We don't spend the same mental effort on "2+2=?" as we do on proving a theorem. The model shouldn't either.

---

*This README summarizes the first 16 pages of the paper. For complete details, including appendices and additional experimental results, please refer to the full paper.*

---

## üéì Learning Path

**For those new to test-time compute:**

1. **Start here**: Understand [Core Concepts](#core-concepts) - Proposer/Verifier framework
2. **See it work**: Check [Results Summary](#results-summary) - What's possible
3. **Get intuition**: Read [Common Questions](#common-questions--troubleshooting) - Why things work
4. **Go deeper**: Study [Methodology](#methodology) - How to implement
5. **Plan deployment**: Use [Practical Implementation Guide](#practical-implementation-guide)

**For researchers:**
- Deep dive: [Methodology](#methodology) + [Architecture Components](#architecture-components)
- Baselines: [Experimental Setup](#experimental-setup)
- Novel contributions: Compute-optimal strategy, difficulty-dependent performance

**For practitioners:**
- Quick start: [Practical Implementation Guide](#practical-implementation-guide)
- Cost analysis: [Test-Time vs. Pretraining Compute Tradeoff](#2-test-time-vs-pretraining-compute-tradeoff)
- Troubleshooting: [Common Questions](#common-questions--troubleshooting)

---

## üìä Quick Reference: Method Selection

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DECISION TREE: Which method should I use?                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Q1: What's your inference-to-pretraining ratio (R)?
‚îú‚îÄ R >> 1 (Production scale)
‚îÇ  ‚îî‚îÄ Consider larger pretrained model (latency matters)
‚îÇ
‚îú‚îÄ R ‚âà 1 (Balanced)
‚îÇ  ‚îú‚îÄ Q2: What's the difficulty distribution?
‚îÇ  ‚îÇ  ‚îú‚îÄ Mostly easy/medium ‚Üí Test-time compute
‚îÇ  ‚îÇ  ‚îî‚îÄ Mostly hard ‚Üí Larger model
‚îÇ
‚îî‚îÄ R << 1 (Self-improvement)
   ‚îî‚îÄ Q2: What's question difficulty?
      ‚îú‚îÄ Easy (pass@1 > 60%)
      ‚îÇ  ‚îî‚îÄ Use: Sequential revisions (32-128 steps)
      ‚îÇ     Budget: Low (fast refinement)
      ‚îÇ
      ‚îú‚îÄ Medium (pass@1: 30-60%)
      ‚îÇ  ‚îî‚îÄ Use: Balanced revisions (16:8 seq:par)
      ‚îÇ     Budget: Medium (exploration + refinement)
      ‚îÇ
      ‚îú‚îÄ Hard (pass@1: 10-30%)
      ‚îÇ  ‚îî‚îÄ Use: Beam search or Best-of-N
      ‚îÇ     Budget: High (need many samples)
      ‚îÇ
      ‚îî‚îÄ Very Hard (pass@1 < 10%)
         ‚îî‚îÄ Use: Larger model (test-time won't help much)

VERIFIER QUALITY CHECK:
‚îú‚îÄ Good (>75% accuracy) ‚Üí Use PRM search methods
‚îú‚îÄ Okay (60-75%) ‚Üí Use best-of-N with weighted voting
‚îî‚îÄ Poor (<60%) ‚Üí Stick to majority voting

LATENCY CONSTRAINTS:
‚îú‚îÄ <1s ‚Üí Simple sampling or larger model
‚îú‚îÄ 1-5s ‚Üí Modest test-time compute (16-32 samples)
‚îî‚îÄ >5s ‚Üí Full compute-optimal strategy (64-256 samples)
```

---

## üöÄ Getting Started: 30-Minute Implementation

**Minimal working example** - Add test-time compute to your LLM:

**Algorithm: Simple Best-of-N with Majority Voting**
```
Input: Model M, prompt q, n_samples = 8
Output: Best answer (no verifier needed!)

1: // Generate multiple samples
2: samples ‚Üê []
3: for i ‚Üê 1 to n_samples do
4:     sample ‚Üê M.Generate(q, temperature=0.8)
5:     samples.Append(sample)
6: end for

7: // Extract final answers
8: answers ‚Üê []
9: for each s ‚àà samples do
10:    ans ‚Üê ExtractAnswer(s)  // e.g., regex, last line, etc.
11:    answers.Append(ans)
12: end for

13: // Majority vote
14: return MostCommon(answers)

Note: Improvement over single sample: ~20-30% on many tasks
```

**Level 2: Add simple verifier**

**Algorithm: Best-of-N with Simple Heuristic Verifier**
```
Input: Model M, prompt q, n_samples = 16
Output: Best answer

1: samples ‚Üê M.Generate(q, n=n_samples)

2: // Simple verifier: heuristic scoring
3: for each s ‚àà samples do
4:     score[s] ‚Üê 0.0
5:     
6:     // Longer solutions often better (up to a point)
7:     words ‚Üê CountWords(s)
8:     score[s] ‚Üê score[s] + min(words, 100) / 100
9:     
10:    // Contains reasoning keywords
11:    keywords ‚Üê {"because", "therefore", "thus", "so"}
12:    for each kw ‚àà keywords do
13:        if kw ‚àà Lowercase(s) then
14:            score[s] ‚Üê score[s] + 0.25
15:        end if
16:    end for
17:    
18:    // Has final answer marker
19:    if "final answer" ‚àà Lowercase(s) then
20:        score[s] ‚Üê score[s] + 0.5
21:    end if
22: end for

23: // Select highest scoring sample
24: return argmax_s score[s]
```

**Achievement unlocked**: You've now implemented the core ideas from this paper at a basic level!

---

*Last updated: Based on arXiv:2408.03314v1 (August 2024)*
