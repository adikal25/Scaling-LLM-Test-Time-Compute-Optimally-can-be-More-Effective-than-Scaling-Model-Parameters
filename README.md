

# Scaling LLM Test-Time Compute Optimally

**Authors:** Charlie SnellÂ¹, Jaehoon LeeÂ², Kelvin XuÂ², Aviral KumarÂ²
**Affiliations:** Â¹UC Berkeleyâ€ƒÂ²Google DeepMind
**Published:** August 7 2024
**Paper:** [arXiv:2408.03314](https://arxiv.org/abs/2408.03314)

---

## ðŸ§­ Overview

### Core Problem

Large language models (LLMs) generally improve with scaleâ€”but so do **compute cost, memory, and latency**.
This work asks:

> *Can a smaller model match or beat a much larger one if we spend more compute at inference time instead of training time?*

### Approach Summary

The authors design a **unified framework** that allocates inference-time compute between:

* **Proposer (input-side):** sequential or revision-based generation of answers.
* **Verifier (output-side):** a *Process Reward Model (PRM)* that scores intermediate reasoning steps and guides search (best-of-N, beam, or lookahead).

A **difficulty estimator** dynamically splits the compute budget across these mechanisms.
The base model (PaLM-2-S*) operates under a fixed FLOPs budget.

### Headline Finding

A compute-optimal policy yields â‰ˆ **4Ã— efficiency improvement** over static baselines.
Under FLOPs-matched conditions, a **small model + smart inference** can outperform a **model â‰ˆ 14Ã— larger**.

---

## ðŸ” Motivation & Prior Work

Earlier methods such as *Self-Refine*, *Multi-Agent Debate*, *Best-of-N Sampling*, and *Beam Search* showed partial gains, but lacked a **unified compute-scaling perspective**.
Snell et al. (2024) fill this gapâ€”proposing a principled framework that connects all these inference-time strategies and explains when each is optimal.

---

## ðŸ§© Unified Framework: Proposer Ã— Verifier

| Component          | Goal                                                                     | Effective When                             |
| ------------------ | ------------------------------------------------------------------------ | ------------------------------------------ |
| **Proposer**       | Sequential revisionsâ€”generate and refine answers in context              | Model is already near correct (EASY tasks) |
| **Verifier (PRM)** | Score partial reasoning steps and final answers via Monte-Carlo rollouts | Exploration matters (HARD tasks)           |

Together they enable **adaptive test-time compute**:
allocate more sequential revisions for easy prompts, and more parallel search for hard ones.

---

## ðŸ— Architecture & Algorithms

### System Components

* **Base Model (M):** PaLM 2-S*.
* **Revision Model (M_rev):** Finetuned for self-correction.
* **Process Reward Model (V):** Verifier trained without human labels.
* **Strategy Selector:** Chooses method and budget based on difficulty estimate.

---

### **Algorithm 1 â€” Process Reward Model Training**

```pseudocode
Input: base model M, training questions Q
hyperparameters: n_s = 16 samples per q, n_r = 16 rollouts per step
Output: process reward model V

D â† âˆ…
for q âˆˆ Q:
    S â† { M(q) | i = 1..n_s }              # generate candidate solutions
    for s âˆˆ S:
        steps â† split(s)                   # decompose into reasoning steps
        for each prefix p up to step_i:
            successes â† 0
            for j = 1..n_r:
                c â† M(Â· | p)               # rollout continuation
                if IsCorrect(c, q): successes â† successes + 1
            y_i â† successes / n_r
            D â† D âˆª {(q, p, step_i, y_i)}
train V on D with binary cross-entropy loss:
    L = âˆ’Î£_i [ y_i log rÌ‚_i + (1 âˆ’ y_i) log (1 âˆ’ rÌ‚_i) ]
return V
```

*Innovation:* no human labels neededâ€”Monte-Carlo rollouts supply step-level rewards.

---

### **Algorithm 2 â€” Best-of-N (Weighted by Verifier)**

```pseudocode
Input: sample set S = {sâ‚,â€¦,s_N}, verifier V
Output: best answer Ã¢

group samples by final answer a
for each group G[a]:
    score[a] â† Î£_{s âˆˆ G[a]} V(s)
return Ã¢ â† argmax_a score[a]
```

---

### **Algorithm 3 â€” Beam Search with PRM**

```pseudocode
Input: model M, verifier V, question q, budget N, beam width m, max steps T
Output: best solution

B â† { M_step(q) : i = 1..N }               # initial beams
for t = 1..T:
    if AllComplete(B): break
    r_b â† V(b) for each b âˆˆ B
    B_top â† TopK(B, r_b, k = N/m)
    Bâ€² â† âˆ…
    for b âˆˆ B_top:
        E â† { M_continue(b) : i = 1..m }
        Bâ€² â† Bâ€² âˆª E
    B â† Bâ€²
return BestOfNWeighted(B, V)
```

---

### **Algorithm 4 â€” Revision Model Training**

```pseudocode
Input: M, training questions Q, n_s = 64
Output: revision model M_rev

T â† âˆ…
for q âˆˆ Q:
    S â† { M(q) : i = 1..n_s }
    S_correct â† { s | IsCorrect(s, q) }
    S_incorrect â† S \ S_correct
    for s_c âˆˆ S_correct:
        k âˆ¼ Uniform({0,â€¦,4})
        if k = 0:
            Ï„ â† [s_c]
        else:
            s_last â† argmin_{s âˆˆ S_incorrect} edit_distance(s, s_c)
            S_other â† RandomSample(S_incorrect \ {s_last}, k âˆ’ 1)
            Ï„ â† [S_other, s_last, s_c]
        T â† T âˆª {(q, Ï„)}
finetune M on T via supervised learning
return M_rev
```

---

### **Algorithm 5 â€” Compute-Optimal Strategy Selection**

```pseudocode
Input: question q, models (M, M_rev), verifier V, budget N
Output: final answer Ã¢

# Estimate difficulty
P â† { M(q) : i = 1..16 }
rÌ„ â† mean(V(s) for s âˆˆ P)
if rÌ„ > 0.60: d â† EASY
elif rÌ„ > 0.35: d â† MEDIUM
elif rÌ„ > 0.15: d â† HARD
else: d â† VERY_HARD

# Select method and compute split
if d = EASY: â€ƒ Ã¢ â† SequentialRevisions(M_rev, V, N)
elif d = MEDIUM:  Ã¢ â† MixedSearch(M_rev, M, V, N)
elif d = HARD: â€ƒ  Ã¢ â† ParallelSearch(M, V, N)
else: â€ƒâ€ƒâ€ƒ      Ã¢ â† BeamSearch(M, V, N, beam_width = 4)
return Ã¢
```

---

## ðŸ“Š Experimental Findings

* **Dataset:** [MATH](https://github.com/hendrycks/math) benchmark with graded difficulty.
* **Model:** PaLM 2-S*.

**Results:**

| Difficulty | Optimal Method           | Efficiency Gain |
| ---------- | ------------------------ | --------------- |
| Easy       | Sequential Revisions     | â‰ˆ 4Ã—            |
| Medium     | Hybrid Mix               | â‰ˆ 3Ã—            |
| Hard       | Parallel Verifier Search | â‰ˆ 4Ã—            |

* **Beam Search Limitation:** For easy tasks, accuracy drops at high budgets (over-optimization).
* **FLOPs-Matched Trade-off:** Test-time compute wins when inference tokens â‰ª pretraining tokens.

---

## ðŸ§  Discussion & Critical Analysis

**Strengths**

* First formal definition of compute-optimal inference.
* Demonstrates quantitative scaling laws for test-time compute.

**Limitations**

* Difficulty estimation adds â‰ˆ8Ã— hidden compute cost.
* Verifier bias limits generalization.
* Results evaluated only on math reasoning.
* Revision model sometimes oscillates (wrong â†’ correct â†’ wrong).

**Takeaway:** Sound theory, but needs lighter difficulty estimation and cross-domain tests.

---

## ðŸŒ Impact & Significance

1. **Paradigm Shift:** Performance = (smaller model + smarter inference).
2. **System Design:** Adaptive compute routing for cost-efficient LLM deployment.
3. **Research Influence:** Foundation for OpenAI *o1* and DeepSeek *R1* reasoning systems.
4. **Economic Impact:** Lower inference FLOPs per task â†’ broader accessibility.

---

## ðŸ’¬ Questions for the Audience

1. You have 64 calls to use. For a nearly-correct answer, do you choose sequential revisions or parallel search â€” and why?
2. Why does beam search sometimes hurt easy questions under PRM guidance?

---

## ðŸ“š Resources & Further Reading

* **Paper:** Snell et al. (2024) *Scaling LLM Test-Time Compute Optimally.*
* **Dataset:** [MATH Benchmark](https://github.com/hendrycks/math)
* **Review:** [Yannic Kilcher Video](https://www.youtube.com/watch?v=AfAmwIP2ntY)
* **Related:** OpenAI â€œLetâ€™s Verify Step by Stepâ€; DeepSeek R1 Replication.

---

## ðŸ“– Citation

```bibtex
@article{snell2024scaling,
  title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}
```

---

## ðŸ—‚ Repository Structure & Presentation Notes

```
README.md        # this file (primary presentation)
figures/         # figure images (optional)
notebooks/       # optional code demos
```

### Presentation Checklist

âœ… Screen share tested
âœ… Font zoom for visibility
âœ… â‰¤ 15 minutes runtime + Q&A
âœ… Two audience questions ready

---

## ðŸ§© Key Takeaway

> Scaling parameters is not the only path to better LLMs.
> **Strategic allocation of inference computeâ€”guided by difficulty and verifiersâ€”can match or exceed larger models at a fraction of the cost.**


