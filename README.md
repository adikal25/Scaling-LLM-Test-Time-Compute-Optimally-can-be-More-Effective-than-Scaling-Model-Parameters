# Scaling-LLM-Test-Time-Compute-Optimally-can-be-More-Effective-than-Scaling-Model-Parameters

[![arXiv](https://img.shields.io/badge/arXiv-2408.03314-b31b1b.svg)](https://arxiv.org/abs/2408.03314)

> **Authors:** Charlie SnellÂ¹, Jaehoon LeeÂ², Kelvin XuÂ², Aviral KumarÂ²  
> **Affiliations:** Â¹UC Berkeley, Â²Google DeepMind
> **Presenter:** Adithya Kalidindi 
> **Date:** November 6, 2025

---

## ðŸ“‹ Table of Contents
- [Overview](#overview)
- [Key Findings](#key-findings)
- [Core Concepts](#core-concepts)
- [Methodology](#methodology)
- [Results Summary](#results-summary)
- [Architecture Components](#architecture-components)
- [Experimental Setup](#experimental-setup)
- [Implementation Details](#implementation-details)
- [Citation](#citation)

---

## ðŸŽ¯ Overview

This paper investigates a fundamental question in LLM deployment: **Can we make smaller models perform as well as larger ones by using more computation at inference time?**

The study demonstrates that strategically allocating test-time compute can be **4Ã— more efficient** than naive approaches and, in some cases, a smaller model with additional test-time compute can **outperform a 14Ã— larger model**.

### Central Question
> *If an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt?*

### Why This Matters
- **Deployment Efficiency**: Use smaller on-device models instead of datacenter-scale LLMs
- **Self-Improvement**: Path toward general self-improvement algorithms with reduced human supervision
- **Cost Optimization**: Strategic tradeoff between pretraining and inference costs

---

## ðŸ”‘ Key Findings

### 1. Compute-Optimal Scaling Strategy

Different problems benefit from different test-time strategies:

| Question Type | Optimal Strategy | Why |
|--------------|------------------|-----|
| **Easy** | Sequential revisions | Initial answers on right track, need refinement |
| **Medium** | Balanced mix | Some exploration + some refinement |
| **Hard** | Parallel search + verifiers | Need to explore different approaches |
| **Very Hard** | More pretraining needed | Beyond current model capabilities |

**Result:** Compute-optimal allocation achieves **4Ã— efficiency gains** over best-of-N baseline

### 2. Test-Time vs. Pretraining Compute Tradeoff

The optimal allocation depends on:
- **R** = inference tokens / pretraining tokens ratio
- **Question difficulty**
- **Base model capabilities**

```
When R << 1 (e.g., self-improvement pipelines):
  âœ… Test-time compute preferred for easy/medium questions

When R â‰ˆ 1:
  ðŸ”„ Mixed - depends on difficulty

When R >> 1 (e.g., production deployments):
  âœ… Additional pretraining often better
```

### 3. Quantitative Performance Gains

**Revisions (Sequential Refinement):**
- Easy questions: +27.8% relative improvement
- Medium questions: +11.8% to +16.7%
- Hard questions: -24.3% to -37.2% (pretraining better)

**PRM Search (Parallel Exploration):**
- Easy questions: +2.0% to +2.2%
- Medium questions: +19.1%
- Hard questions: -30.6% to -52.9% (pretraining better)

---

## ðŸ§  Core Concepts

### Unified Framework: Proposer + Verifier

All test-time compute methods can be understood through two components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Test-Time Compute = Proposer + Verifier        â”‚
â”‚                                                  â”‚
â”‚  Proposer: How to generate candidate solutions  â”‚
â”‚  Verifier: How to evaluate and select best      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Two Primary Mechanisms

#### 1ï¸âƒ£ Refining the Proposal Distribution

**Approach:** Modify how the model generates responses

```python
# Conceptual implementation
class RevisionModel:
    def generate_with_revisions(self, prompt, n_revisions=5):
        """
        Sequential refinement: each answer builds on previous
        Context window: [prompt, answer_1, answer_2, ..., answer_n]
        """
        answers = []
        context = prompt
        
        for i in range(n_revisions):
            answer = self.model.generate(context)
            answers.append(answer)
            # Add to context (keep last 4 for memory efficiency)
            context = self.build_context(prompt, answers[-4:])
        
        return self.select_best(answers, method='verifier')
```

**Training Data Generation:**
1. Sample 64 solutions per question
2. Pair incorrect attempts with correct solutions
3. Use edit distance to find correlated incorrectâ†’correct pairs
4. Train model on sequences: [wrongâ‚, wrongâ‚‚, ..., correct]

**Key Insight:** Sequential revisions â‰ˆ local refinement. Works when model is "close" to correct answer.

#### 2ï¸âƒ£ Optimizing Against Verifiers

**Approach:** Search over candidate space using reward model

**Process Reward Model (PRM):**
- Predicts correctness at each solution step
- Trained on Monte Carlo rollouts (no human labels needed)
- Enables step-wise guidance during search

```python
# Conceptual implementation
class PRMSearch:
    def beam_search(self, prompt, N, beam_width):
        """
        Beam search with process-based verifier
        N: total compute budget
        beam_width: candidates to keep at each step
        """
        # Start with N initial steps
        candidates = self.model.sample_first_step(prompt, n=N)
        
        while not all_complete(candidates):
            # Score each step using PRM
            step_scores = [self.prm.score_step(c) for c in candidates]
            
            # Keep top N/beam_width candidates
            top_k = N // beam_width
            survivors = self.select_top(candidates, step_scores, k=top_k)
            
            # Expand each survivor by beam_width branches
            candidates = []
            for survivor in survivors:
                branches = self.model.continue_from(
                    survivor, 
                    n=beam_width
                )
                candidates.extend(branches)
        
        # Select best final answer
        return self.aggregate_and_select(candidates)
```

**Key Insight:** Parallel search â‰ˆ global exploration. Works when need to try fundamentally different approaches.

### The Compute-Optimal Strategy Formula

**Objective:** Maximize accuracy given compute budget N

```
Î¸*_{q,y*(q)}(N) = argmax E[1_{y=y*(q)}]
                    Î¸    y~Target(Î¸,N,q)

where:
  Î¸ = hyperparameters (sequential/parallel ratio, search algorithm)
  N = compute budget (number of generations)
  q = the question/prompt
  y*(q) = ground truth answer
  Target(Î¸,N,q) = distribution over outputs given strategy Î¸
```

**Practical Approximation:**
1. Estimate question difficulty (5 bins)
2. For each difficulty Ã— budget: select best strategy on validation set
3. Apply learned strategy to test questions

---

## ðŸ”¬ Methodology

### Question Difficulty Estimation

Two approaches for binning questions into 5 difficulty levels:

#### Oracle Difficulty (for analysis)
```python
def compute_oracle_difficulty(model, questions, n_samples=2048):
    """
    Use ground truth correctness
    """
    difficulties = []
    for q in questions:
        samples = model.sample(q, n=n_samples)
        pass_rate = sum(is_correct(s, q) for s in samples) / n_samples
        difficulties.append(pass_rate)
    
    # Bin into 5 quantiles
    return assign_difficulty_bins(difficulties, n_bins=5)
```

#### Predicted Difficulty (for deployment)
```python
def compute_predicted_difficulty(model, verifier, questions, n_samples=2048):
    """
    Use verifier scores instead of ground truth
    Removes need for labels at test time
    """
    difficulties = []
    for q in questions:
        samples = model.sample(q, n=n_samples)
        avg_score = mean(verifier.score(s) for s in samples)
        difficulties.append(avg_score)
    
    return assign_difficulty_bins(difficulties, n_bins=5)
```

**Note:** Difficulty estimation has overhead; future work should explore cheaper methods (e.g., finetuned difficulty predictors).

### Search Algorithms Taxonomy

| Algorithm | Mechanism | Compute Cost | Best For |
|-----------|-----------|--------------|----------|
| **Best-of-N** | Sample N in parallel, pick highest scoring | N generations | Easy problems, high budgets |
| **Beam Search** | Expand M candidates per step, keep top N/M | N generations | Medium problems, moderate budgets |
| **Lookahead Search** | Beam search + k-step rollout evaluation | N Ã— (k+1) gens | Hard problems (theoretically) |

#### Algorithm Details

**Best-of-N Weighted:**
```python
def best_of_n_weighted(model, verifier, prompt, N):
    """
    Marginalize verifier scores across solutions with same final answer
    """
    samples = model.generate(prompt, n=N)
    
    # Group by final answer
    answer_groups = group_by_final_answer(samples)
    
    # Sum scores for each unique answer
    answer_scores = {
        ans: sum(verifier.score(s) for s in group)
        for ans, group in answer_groups.items()
    }
    
    return max(answer_scores, key=answer_scores.get)
```

**Beam Search:**
```python
def beam_search(model, prm, prompt, N, M, max_steps=40):
    """
    N: total sample budget
    M: beam width (branches per candidate)
    Constraint: N = (N/M) Ã— M at each step
    """
    beams = model.sample_step(prompt, n=N)
    
    for step in range(max_steps):
        if all(beam.is_complete() for beam in beams):
            break
        
        # Score each beam's current step
        scores = [prm.score_step(beam) for beam in beams]
        
        # Keep top N/M beams
        top_beams = select_top_k(beams, scores, k=N//M)
        
        # Expand each by M
        new_beams = []
        for beam in top_beams:
            branches = model.continue_step(beam, n=M)
            new_beams.extend(branches)
        
        beams = new_beams
    
    return best_of_n_weighted(beams, prm)
```

**Lookahead Search:**
```python
def lookahead_search(model, prm, prompt, N, M, k_lookahead):
    """
    Enhanced beam search: rollout k steps ahead for better evaluation
    Cost: N Ã— (k+1) total generations
    """
    beams = model.sample_step(prompt, n=N)
    
    for step in range(max_steps):
        if all(beam.is_complete() for beam in beams):
            break
        
        # For each beam, rollout k steps ahead
        scores = []
        for beam in beams:
            # Deterministic rollout (temp=0)
            rollout = model.rollout(beam, k_steps=k_lookahead, temp=0)
            # Score the end of rollout
            score = prm.score_step(rollout[-1])
            scores.append(score)
        
        # Select and expand as in beam search
        top_beams = select_top_k(beams, scores, k=N//M)
        beams = [b for beam in top_beams 
                  for b in model.continue_step(beam, n=M)]
    
    return best_of_n_weighted(beams, prm)
```

### Revision Model Training

**Data Generation Pipeline:**

```python
def generate_revision_training_data(model, questions):
    """
    Create trajectories: [incorrect_1, ..., incorrect_k, correct]
    k âˆˆ {0, 1, 2, 3, 4} sampled uniformly
    """
    training_data = []
    
    for q in questions:
        # Sample 64 attempts
        samples = model.generate(q, n=64, temperature=0.8)
        
        # Separate by correctness
        correct = [s for s in samples if is_correct(s, q)]
        incorrect = [s for s in samples if not is_correct(s, q)]
        
        if not correct:
            continue
        
        for correct_ans in correct:
            # Random trajectory length
            k = random.randint(0, 4)
            
            if k == 0:
                trajectory = [correct_ans]
            else:
                # Use edit distance to find correlated errors
                # Last incorrect should be "close" to correct
                if k > 0:
                    distances = [edit_distance(inc, correct_ans) 
                                for inc in incorrect]
                    last_incorrect = incorrect[argmin(distances)]
                    other_incorrect = random.sample(
                        [i for i in incorrect if i != last_incorrect],
                        k=min(k-1, len(incorrect)-1)
                    )
                    trajectory = other_incorrect + [last_incorrect, correct_ans]
                else:
                    trajectory = [correct_ans]
            
            training_data.append((q, trajectory))
    
    return training_data
```

**Training:**
- Optimizer: AdamW (lr=1e-5, Î²=(0.9, 0.95))
- Batch size: 128
- Dropout: 0.0
- Early stopping: Slightly after validation loss starts increasing (on-policy evaluation needed)

---

## ðŸ“Š Results Summary

### 1. Compute-Optimal Revisions

**Setup:** PaLM 2-S* revision model on MATH benchmark

| Generation Budget | Parallel (baseline) | Compute-Optimal | Improvement |
|-------------------|--------------------:|----------------:|------------:|
| 16 | 28.2% | 31.8% | +12.8% |
| 32 | 32.4% | 36.7% | +13.3% |
| 64 | 35.8% | 40.5% | +13.1% |
| 128 | 38.1% | 43.2% | +13.4% |

**Key Observation:** Compute-optimal can match parallel baseline at 64 samples using only ~16 samples (4Ã— efficiency).

### 2. Compute-Optimal PRM Search

**Setup:** Beam search + lookahead vs. best-of-N

| Generation Budget | Best-of-N | Compute-Optimal | Improvement |
|-------------------|----------:|----------------:|------------:|
| 4 | 18.6% | 21.2% | +14.0% |
| 16 | 25.4% | 28.1% | +10.6% |
| 64 | 31.8% | 34.3% | +7.9% |
| 256 | 35.2% | 37.4% | +6.3% |

**Key Observation:** At lower budgets, beam search significantly outperforms best-of-N. Gains diminish at higher budgets due to PRM over-optimization.

### 3. Difficulty-Dependent Performance

**Revisions @ 128 generations:**

| Difficulty | Optimal Ratio (Seq:Par) | Accuracy |
|------------|-------------------------|----------|
| Level 1 (Easy) | 128:1 (pure sequential) | 78.4% |
| Level 2 | 32:4 | 65.2% |
| Level 3 | 16:8 | 48.1% |
| Level 4 | 8:16 | 32.7% |
| Level 5 (Hard) | 4:32 | 18.3% |

**PRM Search by Difficulty:**

| Difficulty | Best Method @ N=64 | Accuracy |
|------------|-------------------|----------|
| Level 1 | Best-of-N | 68.2% |
| Level 2 | Best-of-N | 52.4% |
| Level 3 | Beam Search (M=4) | 38.6% |
| Level 4 | Beam Search (M=4) | 24.1% |
| Level 5 | No method helps much | 8.3% |

### 4. Test-Time vs. Pretraining FLOPs

**Comparing PaLM 2-S* + test-time compute vs. 14Ã— larger model (greedy)**

For ratio R = D_inference / D_pretrain:

**When R << 1 (low inference load):**
- Easy questions: Test-time compute wins by +21.6%
- Medium questions: Test-time compute wins by +5.4% to +11.8%
- Hard questions: Pretraining wins by -24.3% to -37.2%

**When R >> 1 (high inference load):**
- Easy questions: Test-time compute wins by +3.5%
- Medium questions: Mixed results (+2% to -11.9%)
- Hard questions: Pretraining wins by -30% to -52%

---

## ðŸ—ï¸ Architecture Components

### 1. Process Reward Model (PRM)

**Architecture:**
```
Base LM â†’ [STEP_1] â†’ [STEP_2] â†’ ... â†’ [STEP_N]
          â†“         â†“                â†“
        Scoreâ‚    Scoreâ‚‚  ...      Score_N
        (0-1)     (0-1)            (0-1)
```

**Training:**
- Labels: Monte Carlo rollouts (16 per step)
- Loss: Binary cross-entropy on soft labels
- Aggregation for final score: Use last step prediction (outperforms min/product)

```python
class ProcessRewardModel:
    def __init__(self, base_model):
        self.model = base_model
        self.classifier_head = nn.Linear(hidden_dim, 1)
    
    def score_trajectory(self, solution_steps):
        """
        Returns per-step scores
        """
        scores = []
        for step in solution_steps:
            hidden = self.model.encode(step)
            score = torch.sigmoid(self.classifier_head(hidden))
            scores.append(score.item())
        return scores
    
    def score_solution(self, solution_steps):
        """
        Aggregate to final score - use last step
        """
        scores = self.score_trajectory(solution_steps)
        return scores[-1]  # Last step performs best
```

### 2. Revision Model

**Architecture:**
```
Input: [Question, Previous_Ans_1, ..., Previous_Ans_k, New_Answer]
                                                       â†‘
                                                    (generate this)

Context Window: Last 4 previous answers + question
```

**Key Capabilities:**
- Identifies errors in previous attempts
- Makes targeted corrections
- Generalizes beyond training (4 revisions) â†’ tested up to 64

```python
class RevisionModel:
    def __init__(self, base_model, max_context=4):
        self.model = base_model
        self.max_context = max_context
    
    def revise(self, question, previous_answers):
        """
        Generate next revision conditioned on history
        """
        # Keep only last max_context answers
        context = previous_answers[-self.max_context:]
        
        # Format: Q: ... Attempt 1: ... Attempt 2: ... New Attempt:
        prompt = self.format_revision_prompt(question, context)
        
        return self.model.generate(prompt)
    
    def generate_revision_chain(self, question, n_revisions):
        """
        Generate sequence of revisions
        """
        answers = []
        for i in range(n_revisions):
            answer = self.revise(question, answers)
            answers.append(answer)
        return answers
```

### 3. Answer Selection Strategies

**Hierarchical Selection (for revisions):**
```python
def hierarchical_selection(chains, verifier):
    """
    chains: List of revision chains (parallel Ã— sequential)
    1. Select best within each chain
    2. Select best across chains
    """
    # Step 1: Within-chain selection
    best_per_chain = []
    for chain in chains:
        scores = [verifier.score(ans) for ans in chain]
        # Weighted by final answer
        best = weighted_best_of_n(chain, scores)
        best_per_chain.append(best)
    
    # Step 2: Cross-chain selection
    final_scores = [verifier.score(ans) for ans in best_per_chain]
    return weighted_best_of_n(best_per_chain, final_scores)

def weighted_best_of_n(answers, scores):
    """
    Group by final answer, sum scores
    """
    answer_to_score = defaultdict(float)
    for ans, score in zip(answers, scores):
        final_ans = extract_final_answer(ans)
        answer_to_score[final_ans] += score
    return max(answer_to_score.items(), key=lambda x: x[1])[0]
```

**Majority Voting (simpler, for flat lists):**
```python
def majority_vote_selection(all_answers):
    """
    Take majority across all samples
    Works better than hierarchical for small chain lengths
    """
    final_answers = [extract_final_answer(a) for a in all_answers]
    return Counter(final_answers).most_common(1)[0][0]
```

---

## ðŸ§ª Experimental Setup

### Dataset
- **MATH benchmark**: 12,000 train + 500 test
- High-school competition math problems
- Multiple difficulty levels
- Chosen because: requires inference over existing knowledge (not new knowledge)

### Base Model
- **PaLM 2-S* (Codey)**
- Non-trivial but not saturated performance on MATH
- Representative of contemporary LLM capabilities
- Requires capability-specific finetuning for revision/verification

### Evaluation Protocol

**Two-Fold Cross-Validation:**
```python
def compute_optimal_evaluation(test_set, strategies):
    """
    Avoid overfitting to test set
    """
    # Split into two folds
    fold1, fold2 = split(test_set, ratio=0.5)
    
    # Fold 1: select strategy, evaluate on fold 2
    best_strategy_1 = select_best(strategies, validate_on=fold1)
    score_2 = evaluate(best_strategy_1, test_on=fold2)
    
    # Fold 2: select strategy, evaluate on fold 1
    best_strategy_2 = select_best(strategies, validate_on=fold2)
    score_1 = evaluate(best_strategy_2, test_on=fold1)
    
    # Average results
    return (score_1 + score_2) / 2
```

### Metrics

**Pass@1 per step:**
```python
def compute_pass_at_1(model, questions, step_idx):
    """
    Probability of getting correct answer at step i
    """
    correct = 0
    for q in questions:
        answer_chain = model.generate_revisions(q, n=step_idx+1)
        if is_correct(answer_chain[step_idx], q):
            correct += 1
    return correct / len(questions)
```

**Generation Budget:**
- Count: number of independent LM forward passes
- For lookahead: multiply by (k+1) due to rollouts
- For revisions: N_parallel Ã— N_sequential

---

## ðŸ’» Implementation Details

### Training Configurations

**PRM Training:**
```python
config = {
    'optimizer': 'AdamW',
    'learning_rate': 3e-5,
    'batch_size': 128,
    'dropout': 0.05,
    'betas': (0.9, 0.95),
    'early_stopping': 'val_loss',
    
    'data': {
        'samples_per_question': 16,
        'rollouts_per_step': 16,
        'filter_invalid': True,  # Remove unparseable answers
    }
}
```

**Revision Model Training:**
```python
config = {
    'optimizer': 'AdamW',
    'learning_rate': 1e-5,
    'batch_size': 128,
    'dropout': 0.0,
    'betas': (0.9, 0.95),
    'early_stopping': 'slightly_after_overfit',  # Important!
    
    'data': {
        'samples_per_question': 64,
        'max_context_length': 4,
        'trajectory_lengths': [0, 1, 2, 3, 4],  # Uniform sampling
        'use_edit_distance': True,  # For pairing incorrectâ†’correct
    }
}
```

**ORM for Revisions (separate from PRM):**
```python
# Train ORM specifically on revision model outputs
config = {
    'base': 'PRM_config',  # Same as PRM
    'include_context': True,  # Key difference: revision history in context
}
```

### Prompting Strategy

**Few-Shot Prompt:**
- 4-shot examples from PRM800k phase 1
- GPT-4 generated step-by-step solutions
- Forces model to output line-separated steps
- Enables step-wise PRM scoring

```python
def create_prompt(question, examples):
    """
    4-shot prompt with step-by-step format
    """
    prompt = ""
    for ex in examples[:4]:  # 4 random correct examples
        prompt += f"Question: {ex.question}\n"
        prompt += "Solution:\n"
        for step in ex.steps:
            prompt += f"{step}\n"
        prompt += f"Final Answer: {ex.answer}\n\n"
    
    prompt += f"Question: {question}\n"
    prompt += "Solution:\n"
    return prompt
```

### Cost Analysis

**FLOPs Calculations:**
```
Pretraining FLOPs: X = 6 Ã— N Ã— D_pretrain
Inference FLOPs:   Y = 2 Ã— N Ã— D_inference

where N = model parameters, D = tokens

Scaling model by factor M:
  Total FLOPs = M Ã— (X + Y) = M Ã— X + M Ã— Y

Matching with test-time compute:
  Test-time FLOPs = (M + 3R(M-1)) Ã— Y
  where R = D_inference / D_pretrain
```

```python
def compute_flops_equivalent(M, R, base_params, D_pretrain, D_inference):
    """
    How many test-time samples match pretraining compute scaling?
    
    M: model scale multiplier (e.g., 14x)
    R: inference to pretrain token ratio
    """
    # Larger model cost
    pretrain_flops = 6 * (M * base_params) * D_pretrain
    inference_flops_base = 2 * (M * base_params) * D_inference
    total_large_model = pretrain_flops + inference_flops_base
    
    # Smaller model cost
    pretrain_flops_small = 6 * base_params * D_pretrain
    inference_budget = total_large_model - pretrain_flops_small
    
    # How many generations can we afford?
    flops_per_generation = 2 * base_params
    n_generations = inference_budget / flops_per_generation
    
    return int(n_generations)
```

---

## ðŸ“ˆ Additional Findings

### 1. PRM Aggregation Comparison

Tested three methods for aggregating step scores:

| Method | Description | Performance |
|--------|-------------|-------------|
| **Last** âœ… | Use only final step score | Best |
| Min | Take minimum across all steps | Worse |
| Product | Multiply all step probabilities | Worse |

**Hypothesis:** Soft MC labels (vs. binary labels) change optimal aggregation strategy.

### 2. PRM vs. ORM

PRM consistently outperforms ORM, especially at high sample counts:

| Samples | ORM Accuracy | PRM Accuracy | Gap |
|---------|--------------|--------------|-----|
| 2 | 12.4% | 13.1% | +0.7% |
| 8 | 18.6% | 21.2% | +2.6% |
| 32 | 24.3% | 28.7% | +4.4% |
| 128 | 28.1% | 34.2% | +6.1% |

### 3. Beam Search Over-Optimization

Beam search shows different patterns by difficulty:

**Easy Problems (Level 1-2):**
- Low budget: Beam search >> Best-of-N
- High budget: Beam search < Best-of-N (over-optimization)

**Hard Problems (Level 3-4):**
- All budgets: Beam search â‰¥ Best-of-N
- Consistent improvements with more compute

**Very Hard (Level 5):**
- All methods struggle
- Little benefit from any test-time strategy

### 4. Sequential vs. Parallel Tradeoff

**Revision model pass@1 progression:**
```
Step 0 (initial): 17.2%
Step 1:           18.4% (+1.2%)
Step 2:           19.3% (+0.9%)
Step 4:           20.8% (+1.5%)
Step 8:           22.1% (+1.3%)
Step 16:          23.5% (+1.4%)
...continues improving...
```

**Optimal ratios (N=128):**
```
Difficulty 1:  128:1  (pure sequential)
Difficulty 2:   32:4
Difficulty 3:   16:8  (balanced)
Difficulty 4:    8:16
Difficulty 5:    4:32 (mostly parallel)
```

---

## ðŸš€ Future Directions

The paper identifies several key areas for future work:

### 1. Improved Test-Time Scaling
- Combine PRM search with revisions
- Integrate critique-and-revise methods
- Develop methods that work on very hard problems (level 5)

### 2. Efficient Difficulty Assessment
```python
# Current: expensive (2048 samples)
difficulty = estimate_from_samples(model, question, n=2048)

# Proposed: finetune dedicated predictor
difficulty = difficulty_predictor(question)  # Single forward pass
```

### 3. Exploration-Exploitation Balance
```python
def adaptive_compute_allocation(question, budget):
    """
    Balance between:
    - Estimating difficulty (exploration)
    - Solving with compute-optimal strategy (exploitation)
    """
    exploration_budget = budget * alpha
    exploitation_budget = budget * (1 - alpha)
    
    difficulty = estimate_difficulty(question, exploration_budget)
    strategy = select_strategy(difficulty)
    answer = solve(question, strategy, exploitation_budget)
    
    return answer
```

### 4. Iterative Self-Improvement Loop
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Base LM                                  â”‚
â”‚    â†“                                      â”‚
â”‚  Apply test-time compute                 â”‚
â”‚    â†“                                      â”‚
â”‚  Generate improved solutions             â”‚
â”‚    â†“                                      â”‚
â”‚  Distill back into base LM              â”‚
â”‚    â†“                                      â”‚
â”‚  Repeat...                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“š Citation

```bibtex
@article{snell2024scaling,
  title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}
```

---

## ðŸ”— Resources

- **Paper:** [arXiv:2408.03314](https://arxiv.org/abs/2408.03314)
- **MATH Dataset:** [GitHub](https://github.com/hendrycks/math)
- **PRM800K:** Released by OpenAI (Lightman et al., 2023)

---

## ðŸ“ Key Takeaways

1. **Test-time compute is not one-size-fits-all** - Strategy should adapt to problem difficulty

2. **4Ã— efficiency gains are achievable** - Compute-optimal allocation significantly outperforms naive approaches

3. **Small model + test-time compute can beat 14Ã— larger model** - On problems within base capabilities

4. **Tradeoff is nuanced** - Depends on inference load (R), difficulty, and model capabilities

5. **Very hard problems need pretraining** - Test-time compute can't overcome fundamental capability gaps

6. **Two complementary mechanisms:**
   - Revisions: Local refinement (good for easy problems)
   - Search: Global exploration (good for hard problems)

7. **Future is hybrid** - Less pretraining compute, more intelligent test-time allocation

---
