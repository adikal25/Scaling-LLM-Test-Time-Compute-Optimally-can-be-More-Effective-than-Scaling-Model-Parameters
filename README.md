# Scaling LLM Test-Time Compute Optimally

**Authors:** Charlie SnellÂ¹, Jaehoon LeeÂ², Kelvin XuÂ², Aviral KumarÂ²
**Affiliations:** Â¹UC Berkeley, Â²Google DeepMind
**Published:** August 7, 2024

-----

## ğŸ“‹ Table of Contents

  * [Overview](https://www.google.com/search?q=%23-overview)
  * [The Problem](https://www.google.com/search?q=%23-the-problem-conflicting-literature)
  * [Core Question](https://www.google.com/search?q=%23-core-question)
  * [Unified Framework](https://www.google.com/search?q=%23-unified-framework-proposer--verifier)
  * [Architecture & Algorithms](https://www.google.com/search?q=%23-architecture--algorithms)
  * [Results](https://www.google.com/search?q=%23-results)
  * [Test-Time vs Pretraining](https://www.google.com/search?q=%23-test-time-vs-pretraining)
  * [Critical Analysis](https://www.google.com/search?q=%23-critical-analysis)
  * [Impact](https://www.google.com/search?q=%23-impact--significance)
  * [Questions](https://www.google.com/search?q=%23-questions-for-understanding)
  * [Resources](https://www.google.com/search?q=%23-resources)
  * [Citation](https://www.google.com/search?q=%23-citation)

-----

## ğŸ¯ Overview

> **The Big Idea:** Instead of always training bigger models, can we use more compute at **test-time** to make smaller models perform just as well?This paper investigates how to optimally scale inference-time computation in Large Language Models (LLMs) to improve performance on challenging prompts. The key question: If an LLM can use a fixed amount of test-time compute, how much can it improve?

### What This Paper Shows

  * âœ… **4Ã— efficiency improvement** over naive baselines
  * âœ… **Small model + smart inference** beats **14Ã— larger model** (on appropriate problems)
  * âœ… Strategy must **adapt to question difficulty** - no one-size-fits-all

### Why It Matters

  * **Before:** Better performance = Bigger model (expensive pretraining)
  * **After:** Better performance = Smaller model + adaptive test-time compute (sometimes more efficient)

-----

## âŒ The Problem: Conflicting Literature

### Why This Research Was Needed

The literature showed contradictory results:

| Method | Claim | Reality |
| :--- | :--- | :--- |
| **Self-Refine** | Models can improve by critiquing themselves | âœ— Doesn't work on hard reasoning |
| **Multi-Agent Debate** | Multiple models debating helps | âœ— Just better prompting, not better than majority vote |
| **Best-of-N + Verifier** | Sample many, select best with learned scorer | âœ“ Actually works\! |

### The Gap

No systematic understanding of when/why methods work.
â†’ This paper provides that systematic analysis.

-----

## â“ Core Question

> Given a challenging query, how can we enable LLMs to most effectively use additional test-time computation to improve accuracy?

### Many Possible Approaches

  * **Test-Time Methods:**
      * â”œâ”€ Best-of-N: Sample many, pick best
      * â”œâ”€ Revisions: Iteratively improve answers
      * â”œâ”€ Beam Search: Prune bad paths early
      * â”œâ”€ Verifiers: Score with reward models
      * â””â”€ Combinations

### Key Insight

**Different problems need different strategies\!**

-----

## ğŸ§  Unified Framework: Proposer & Verifier

### Two Independent Mechanisms

1.  **Proposer (Input Level)** - Modify *how* we generate answers

      * **Sequential Revisions:** Model learns from previous attempts
      * **Better for EASY problems** (already on right track)

2.  **Verifier (Output Level)** - Score and *select* answers

      * **Parallel Sampling + Scoring:** Generate many, pick best
      * **Better for HARD problems** (need to explore approaches)

### Visual Example

> **Easy Problem:** "What is 15% of 80?"
> â†’ Use revisions (just fix arithmetic)
>
> **Hard Problem:** "Prove infinitely many primes"
> â†’ Use parallel search (try different proof strategies)

*Caption: Two mechanisms for scaling test-time compute*

-----

## ğŸ—ï¸ Architecture & Algorithms

### Core Components

  * **System Architecture:**
      * Base Model (PaLM 2-S\*)
          * â”œâ”€â†’ Revision Model (finetuned for self-correction)
          * â””â”€â†’ Process Reward Model (PRM, trained on MC rollouts)
              * â†“
      * Compute-Optimal Strategy Selector

### Algorithm 1: Process Reward Model Training

**Input:** Base model $M$, training questions $Q$, samples per question $n_s = 16$, rollouts per step $n_r = 16$
**Output:** Trained PRM $V$

```pseudocode
Initialize training dataset D <- âˆ…
for each question q âˆˆ Q do
    Generate solutions S <- {M(q) : i = 1, ..., n_s}
    for each solution s âˆˆ S do
        Parse into steps: s = [step_1, ..., step_k]
        for i = 1 to k do
            Prefix p <- [step_1, ..., step_i]
            Initialize successes <- 0
            for j = 1 to n_r do
                Sample completion c ~ M(Â· | p)
                if IsCorrect(c, q) then
                    successes <- successes + 1
            end for
            Compute label: y_i <- successes / n_r
            Add to dataset: D <- D âˆª {(q, p, step_i, y_i)}
        end for
    end for
end for

Train PRM V on D with loss:
```

$$
\mathcal{L} = -\sum_i [y_i \log \hat{r}_i + (1-y_i) \log(1-\hat{r}_i)]
$$

```pseudocode
return V
```

*Key Innovation: No human labels needed - uses Monte Carlo rollouts*

### Algorithm 2: Best-of-N Weighted Selection

**Input:** Samples $S = \{s_1, \ldots, s_N\}$, verifier $V$
**Output:** Best answer $\hat{a}$

```pseudocode
Group samples by final answer: G <- GroupByAnswer(S)
Initialize score map: scores <- {}
for each (a, G) âˆˆ G do
    scores[a] <- Î£_{s âˆˆ G} V(s)
end for
return Ã¢ <- argmax_a scores[a]
```

*Note: Marginalizes over all samples with same final answer*

### Algorithm 3: Beam Search with Process Reward Model

**Input:** Model $M$, PRM $V$, question $q$, budget $N$, beam width $M$, max steps $T_{\max}$
**Output:** Best solution

```pseudocode
Initialize beams: B <- {M_step(q) : i = 1, ..., N}
for t = 1 to T_max do
    if AllComplete(B) then break
    
    Score each beam: âˆ€b âˆˆ B: r_b <- V(b)
    Select top beams: B_top <- TopK(B, {r_b}, k = N/M)
    
    Expand beams: B' <- âˆ…
    for each b âˆˆ B_top do
        Sample extensions: E <- {M_continue(b) : i = 1, ..., M}
        B' <- B' âˆª E
    end for
    Update: B <- B'
end for
return BestOfNWeighted(B, V)
```

*Complexity: $O(N \cdot T_{\max})$ generations, $O(N)$ space*

*Caption: Three search methods compared*

### Algorithm 4: Revision Model Training

**Input:** Base model $M$, questions $Q$, samples per question $n_s = 64$
**Output:** Trained revision model $M_{rev}$

```pseudocode
Initialize trajectory set T <- âˆ…
for each q âˆˆ Q do
    Sample solutions: S <- {M(q) : i = 1, ..., n_s}
    Partition: 
        S_correct <- {s âˆˆ S : IsCorrect(s, q)}
        S_incorrect <- S \ S_correct
    
    for each s_c âˆˆ S_correct do
        Sample length: k ~ Uniform({0, 1, 2, 3, 4})
        if k = 0 then
            Trajectory: Ï„ <- [s_c]
        else
            Find similar incorrect: s_last <- argmin_{s âˆˆ S_incorrect} d_edit(s, s_c)
            Sample others: S_other ~ RandomSample(S_incorrect \ {s_last}, k-1)
            Construct: Ï„ <- [S_other, s_last, s_c]
        end if
        T <- T âˆª {(q, Ï„)}
    end for
end for
Finetune M on T using supervised learning
return M_rev
```

*Key Design: Edit distance ensures correlated incorrectâ†’correct transitions*

### Algorithm 5: Revision Chain Generation

**Input:** Revision model $M_{rev}$, question $q$, revisions $n$, context size $k=4$
**Output:** Answer chain $A$

```pseudocode
Initialize A <- []
for i = 1 to n do
    Build context: ctx <- [q, A_{max(0, i-k):i-1}]
    Generate: a_i ~ M_rev(Â· | ctx)
    Append: A <- A âˆª {a_i}
end for
return A
```

*Note: Trained on $\leq 4$ revisions, generalizes to $n > 4$*

### Algorithm 6: Compute-Optimal Strategy Selection

**Input:** Question $q$, models ($M, M_{rev}$), PRM $V$, budget $N$
**Output:** Best answer $\hat{a}$

```pseudocode
// Step 1: Estimate Difficulty
Sample probe set: P <- {M(q) : i = 1, ..., 16}
Compute average score: r_bar <- (1/|P|) * Î£_{s âˆˆ P} V(s)
Map to difficulty:
d <- EASY        if r_bar > 0.6
     MEDIUM      if 0.35 < r_bar <= 0.6
     HARD        if 0.15 < r_bar <= 0.35
     VERY_HARD   otherwise

// Step 2: Select Strategy
if method = "revisions" then
    if d = EASY then (n_seq, n_par) <- (N, 1)
    else if d = MEDIUM then (n_seq, n_par) <- (N/4, 4)
    else if d = HARD then (n_seq, n_par) <- (N/16, 16)
    else (n_seq, n_par) <- (1, N)
    end if
    Execute: Ã¢ <- SolveWithRevisions(q, M_rev, V, n_seq, n_par)

else // method = "search"
    if N < 32 and d âˆˆ {MEDIUM, HARD} then
        Ã¢ <- BeamSearch(M, V, q, N, M=4)
    else if d = EASY then
        Ã¢ <- BestOfN(M, V, q, N) // Avoid over-optimization
    else
        Ã¢ <- BeamSearch(M, V, q, N, M=4)
    end if
end if
return Ã¢
```

*Key Principle: Adapt strategy to estimated difficulty for optimal efficiency*

### Algorithm 7: Hierarchical Answer Selection

**Input:** Revision chains $C = \{C_1, \ldots, C_m\}$ where $C_j = [a_1^j, \ldots, a_n^j]$, verifier $V$
**Output:** Best answer $\hat{a}$

```pseudocode
// Phase 1: Within-chain selection
Initialize B <- []
for j = 1 to m do
    Score chain: âˆ€i: r_i^j <- V(a_i^j)
    Select best: b_j <- BestOfNWeighted(C_j, {r_i^j})
    B <- B âˆª {b_j}
end for

// Phase 2: Cross-chain selection
Score finalists: âˆ€j: r_j <- V(b_j)
return Ã¢ <- BestOfNWeighted(B, {r_j})
```

### Theoretical Framework

**Compute-Optimal Objective:**
Given test-time compute hyperparameters $\theta$, budget $N$, and question $q$, find:

$$
\theta^*(q, N) = \arg\max_\theta \mathbb{E}_{y \sim \text{Target}(\theta, N, q)} [\mathbb{1}_{y = y^*(q)}]
$$where $y^*(q)$ is the ground truth answer.

**Approximation:** Use question difficulty $d(q)$ as sufficient statistic:

$$\\theta^*(q, N) \\approx \\theta^*(d(q), N)
$$Estimate $\theta^*(d,N)$ on validation set, apply to test questions with same difficulty.

-----

## ğŸ“Š Results

### The Compute-Optimal Strategy

**Strategy:** Pick best method based on difficulty

  * **Easy:** Pure revisions (128:1) - *Just needs polish*
  * **Medium:** Mixed (16:8) - *Some refinement, some exploration*
  * **Hard:** Pure parallel (4:32) - *Must find right approach*

*Caption: Optimal strategy varies by question difficulty*

### Performance: 4Ã— Efficiency Gains

Both methods achieve **4Ã— less compute** for same performance:

  * **Revisions:** Compute-optimal @ 64 samples = Parallel baseline @ 256 samples
  * **Search:** Compute-optimal @ 16 samples â‰ˆ Best-of-N @ 64 samples

*Caption: 4Ã— efficiency gains from compute-optimal allocation*

### The Over-Optimization Problem

On **easy** problems, beam search **degrades** at high budgets:

  * **Budget: 4 â†’ 256 samples**
      * **Best-of-N:** 15% â†’ 35% âœ“ (steady)
      * **Beam Search:** 18% â†’ 33% âœ— (peaks then drops)

**Why?** Verifier is mostly right on easy problems. Beam search exploits edge cases.

*Caption: Beam search over-optimizes on easy problems at high budgets*

-----

## ğŸ”„ Test-Time vs Pretraining

### The R Ratio

$$
R = \text{inference\_tokens} / \text{pretraining\_tokens}
$$  * **$R \ll 1$:** Self-improvement â†’ **Test-time wins** on easy/medium
* **$R \gg 1$:** Production â†’ **Pretraining wins** (latency)

### Results Summary

| R Value | Easy | Medium | Hard |
| :--- | :--- | :--- | :--- |
| **$R \ll 1$** | **Test-time +22%** | **Test-time +12%** | Pretraining -24% |
| **$R \gg 1$** | Test-time +4% | Pretraining -12% | Pretraining -36% |

**Key:** Test-time works for problems *within* model capability, but can't overcome fundamental limitations.

*Caption: When test-time compute outperforms scaling model parameters*

-----

## ğŸ” Critical Analysis

### Major Limitations

1.  **Difficulty Estimation Cost:** Requires 2048 samples (\~8Ã— overhead). Not counted in efficiency claims. Real gains may be lower.
2.  **Single Model, Single Domain:** Only PaLM 2-S\* and MATH. May not generalize. Math has unique properties.
3.  **Verifier Dependency:** All gains require good verifier. Over-optimization shows biases. No fallback when verifier poor.
4.  **Distribution Shift:** Trained on [wrongâ†’correct]. Generates [wrongâ†’correctâ†’wrong] 38% of time. Fundamental mismatch.

### Follow-Up Work

* **o1 (OpenAI, Sept 2024):** Shows much larger gains with RL-trained CoT. Suggests paper's methods aren't optimal.
* **DeepSeek R1 (Jan 2025):** Open replication validates RL approach.

**Consensus:** Analysis correct, but RL-trained reasoning (o1/R1) significantly better.

-----

## ğŸŒ Impact & Significance

### How This Changed AI

1.  **Legitimized Test-Time Compute Research:** Systematic framework replacing scattered results. Cited by o1 as foundational.
2.  **Paradigm Shift:**
* **Old:** Performance = Bigger Model
* **New:** Performance = Smaller Model + Smart Inference
3.  **Inspired o1 and R1:**
* This paper: Shows test-time works (Aug 2024)
* o1: Trains models to use it natively (Sept 2024)
* R1: Open replication (Jan 2025)
4.  **Economic Impact:** Enables tiered pricing. Smaller orgs can compete. More sustainable AI.

*Caption: Rapid progression from analysis to practical implementation*

-----

## â“ Questions for Understanding

### Question 1: Strategy Selection

> **For the audience:**
> "Two problems with 64 samples:
> A) Calculate 15% of 80
> B) Prove infinitely many primes
>
> Which strategy for each?
>
>   * Pure parallel (64 independent)?
>   * Pure sequential (64 revisions)?
>   * Mixed (8Ã—8)?"

**Answers:**

* **A (easy):** Pure sequential - just arithmetic check
* **B (hard):** Pure parallel - explore proof strategies

**Key Insight:** Match strategy to difficulty\!

### Question 2: Deployment Decision

> **For the audience:**
> "Coding assistant (10M queries/day), $10M budget:
> A: Train 70B model (10Ã—) - $36.5M/year
> B: Keep 7B + test-time (16x) - $58.4M/year
>
> Which is better?"

**Answer:** Hybrid\!

* Easy (80%) â†’ small + test-time: $0.08/query
* Hard (20%) â†’ large model: $0.20/query
* **Total:** \~$45M/year, best accuracy

**Key Insight:** Route by difficulty\!

-----

## ğŸ“š Resources

* **Paper:** `arXiv:2408.03314`
* **MATH Dataset:** GitHub
* **Yannic Kilcher Review:** YouTube
* **o1 System Card:** OpenAI
* **DeepSeek R1:** GitHub

-----

## ğŸ“– Citation

```bibtex
@article{snell2024scaling,
title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},
author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
journal={arXiv preprint arXiv:2408.03314},
year={2024}
}
```
$$
